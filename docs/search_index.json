[
["index.html", "Clean Air in the Time of COVID Use Cases for Satellite &amp; Sentiment Analysis Overview Motivation Prerequisites", " Clean Air in the Time of COVID Use Cases for Satellite &amp; Sentiment Analysis Ben Best, PhD 2020-06-02 Overview These materials are for a 45 minute teaching demonstration that was on 2020-05-26 oriented towards students in the new Masters in Environmental Data Science at UCSB. Motivation This is an exciting time for the emerging field of Environmental Data Science. Environmental problems are increasingly complex and require advanced technical skills to translate the ever expanding flow of data into actionable insights. Technologies The two specific technologies featured in this teaching demonstration highlight a couple of the most promising techniques to demonstrate big data and machine learning: BIG DATA: Google Earth Engine (satellite) leverages the massive storage and computational capabilities of the Google Cloud to analyze petabytes of the publicly available satellite data. For instance, global climatologies averaging across 40 years of temperature can be calculated and mapped in seconds. This is a truly big data platform! MACHINE LEARNING: TensorFlow (sentiment) is the machine learning software made open-source by Google. It is the most commonly used software for audio, text and image analysis. More specifically tensorflow allows construction of convolutional neural networks, which represent a layering of nodes to enable complex pattern matching. These deep learning models have been popularized by their ability to beat the best human at the most complex game of Go, self-drive vehicles, Netflix recommendations, Spotify playlists and respond to your beck and call through Alexa’s voice commands. Motivating Use Case The improvement in air quality since COVID-19 lockdown was newsworthy. This lab uses cutting edge environmental data science tools to evaluate the physical description and social reaction of this natural experiment. For the first time in decades, Mount Everest was visible from Kathmandu due to improved air quality. — Tom Patterson (@MtnMapper) May 21, 2020 The positive effect of the Lockdown in India’s environment can be seen in New Delhi.This picture depicts the air quality of New Delhi before &amp; after Lockdown.Source - https://t.co/VoG0UbpFzE — GurumaujSatsangi (@GurumaujS) April 23, 2020 Prerequisites See the Setup for required software. For this demonstration to make the most sense, it’s preferable that you’re familiar with: R GIS "],
["setup.html", "Setup 0.1 Apply for accounts 0.2 Install software 0.3 Launch RStudio 0.4 Fork and Clone the Github Repository 0.5 Install R Packages 0.6 Create Rmarkdown file", " Setup 0.1 Apply for accounts 0.1.1 Google Earth Engine (GEE) account Please visit: Google Earth Engine signup You may need to log out and back into your web browser as the preferred Google account to request permissions. This approval process can take days to weeks unfortunately. 0.1.2 Twitter developer account Please visit: Twitter developer signup I recieved an email for clarification and had the account approved and running by the end of the day. 0.2 Install software This workshop will require the following software installed on your machine: R RStudio Please download the appropriate stable release for your operating system. 0.3 Launch RStudio RStudio is an integrated development environment (IDE) for editing text in the code editor, running commands in the R console, viewing defined objects in the workspace and past commands in the history, and viewing plots, files and help. Here’s a layout of the panes in RStudio, each of which can have several tabs for alternate functionality: Check out the Help &gt; Cheatsheets &gt; RStudio IDE Cheat Sheet. 0.4 Fork and Clone the Github Repository If you’d like to play with these materials directly and keep track of differences, please visit: https://github.com/bbest/meds-demo While signed into Github, fork the repo into your own writable user space so you can clone it in RStudio. Then you can follow along in RStudio by evaluating the chunks of R code, while referencing this website: http://benbestphd.com/meds-demo 0.5 Install R Packages Here’s a bit of code to ensure you have all the necessary packages installed. Please copy and paste this code into your R Console. # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) pkgs &lt;- c( # utility &quot;here&quot;,&quot;glue&quot;, &quot;readr&quot;,&quot;dplyr&quot;,&quot;tidyr&quot;,&quot;purrr&quot;,&quot;scales&quot;, &quot;lubridate&quot;,&quot;stringr&quot;,&quot;units&quot;, # api &quot;jsonlite&quot;, # plotting &quot;ggplot2&quot;,&quot;plotly&quot;, # spatial &quot;sf&quot;, # text &quot;rtweet&quot;,&quot;tidytext&quot;,&quot;textdata&quot;, # tensorflow &quot;tensorflow&quot;,&quot;keras&quot;) shelf(pkgs) # report on versions of software &amp; packages sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.5 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] keras_2.3.0.0 tensorflow_2.2.0 textdata_0.4.1 tidytext_0.2.4 ## [5] rtweet_0.7.0 sf_0.9-3 plotly_4.9.2.1 ggplot2_3.3.1 ## [9] jsonlite_1.6.1 units_0.6-6 stringr_1.4.0 lubridate_1.7.8 ## [13] scales_1.1.1 purrr_0.3.4 tidyr_1.1.0 dplyr_1.0.0 ## [17] readr_1.3.1 glue_1.4.1 here_0.1 librarian_1.7.0 ## ## loaded via a namespace (and not attached): ## [1] reticulate_1.16 tidyselect_1.1.0 xfun_0.14 lattice_0.20-41 ## [5] colorspace_1.4-1 vctrs_0.3.0 generics_0.0.2 SnowballC_0.7.0 ## [9] htmltools_0.4.0 viridisLite_0.3.0 base64enc_0.1-3 yaml_2.2.1 ## [13] rlang_0.4.6 e1071_1.7-3 pillar_1.4.4 withr_2.2.0 ## [17] DBI_1.1.0 lifecycle_0.2.0 munsell_0.5.0 gtable_0.3.0 ## [21] htmlwidgets_1.5.1 evaluate_0.14 knitr_1.28 tfruns_1.4 ## [25] class_7.3-16 tokenizers_0.2.1 Rcpp_1.0.4.6 KernSmooth_2.23-16 ## [29] backports_1.1.7 classInt_0.4-3 fs_1.4.1 hms_0.5.3 ## [33] digest_0.6.25 stringi_1.4.6 bookdown_0.19 rprojroot_1.3-2 ## [37] grid_4.0.0 tools_4.0.0 magrittr_1.5 lazyeval_0.2.2 ## [41] tibble_3.0.1 janeaustenr_0.1.5 zeallot_0.1.0 whisker_0.4 ## [45] crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.1 Matrix_1.2-18 ## [49] data.table_1.12.8 rmarkdown_2.2 httr_1.4.1 rstudioapi_0.11 ## [53] R6_2.4.1 compiler_4.0.0 0.6 Create Rmarkdown file Rmarkdown is a dynamic document format that allows you to knit chunks of R code with formatted text (aka markdown). We recommend that you use this format for keeping a reproducible research document as you work through the lessons To get started, File &gt; New File &gt; Rmarkdown… and go with default HTML document and give any title you like (default “Untitled” or “test” or “First Rmd” is fine). Check out the Help &gt; Markdown Quick Reference and Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet. Here’s a 1 minute video on the awesomeness of Rmarkdown: "],
["satellite.html", "Lesson 1 Satellite Objectives Prerequisites 1.1 Get city boundary 1.2 Browse datasets, tag no2 1.3 View dataset info 1.4 Questions 1.5 Launch Code Editor with dataset 1.6 Run the initial dataset script 1.7 Modify the script so satellite layer is semi-transparent 1.8 Add city polygon asset 1.9 Add to city to map 1.10 Create time series chart 1.11 Expand dates to all available 1.12 Download CSV 1.13 Your Turn Conclusions Further Resources", " Lesson 1 Satellite Objectives Question How have emissions related to air quality changed since COVID-19 lockdowns were put in place? Background. Nitrogen dioxide (NO2) is produced from burning of fossil fuel, especialy from motor vehicles, and contributes to the formation of other air pollutants, including ozone, particulate mattter and acid rain. Study area: Delhi, India We’ll use Delhi, India as our initial city study area. Prime Minister Modhi issued a nationwide lockdown on 24 March, 2020. News: Air pollution falls by unprecedented levels in major global cities during coronavirus lockdowns - CNN India: air pollution in the north has hit a 20-year low, NASA report says - CNN Fact Check: Is The COVID-19 Lockdown Decreasing Delhi Air Pollution? - Smart Air Filters Learning outcomes The student will learn how to: Browse Google Earth Engine’s data catalog Examine basic information about the dataset Map datasets in the Code Editor Upload assets, such as a polygon Adjust time period for averaging Generate a time series chart for satellite data extracted to a region Download data as a text file (csv) Prerequisites A Google Earth Engine account that is associated with a Google account, such as from Gmail, is required to log into https://code.earthengine.google.com. If you need a GEE account, please visit https://signup.earthengine.google.com. You may need to log out and back into your web browser as the preferred Google account to request permissions. This approval process can take days to weeks unfortunately. 1.1 Get city boundary The first step is to define our study area. I made a little R helper function city2zip() to: Fetch the administrative boundary from the Nominatim OpenStreetMap API given a city name. Extract the polygon information, convert to shapefile and zip for upload into GEE. source(&quot;functions.R&quot;) loc_name &lt;- &quot;delhi&quot; city2zip(&quot;Delhi, India&quot;, loc_name) ## $geojson ## [1] &quot;/Users/bbest/github/meds-demo/data/city_delhi.geojson&quot; ## ## $zip ## [1] &quot;/Users/bbest/github/meds-demo/data/city_delhi.zip&quot; The function returns the paths to files generated. You will use this zip file to upload into GEE as an asset. 1.2 Browse datasets, tag no2 Visit https://earthengine.google.com &gt; Datasets (upper right). Be sure to explore the many datasets available here. Since we know we want Nitrogen Dioxide (NO2), click on Browse by tags and Filter by “no2”. You should see two datasets: Screenshot of GEE data catalog filtered by tag “no2” showing two datasets. 1.3 View dataset info Please click on “Sentinel-5P NRTI NO2: Near Real-Time Nitrogen Dioxide” to get the dataset view. Explore the metadata for this dataset. 1.4 Questions How many years of data are available? How does this compare with the other “Offline” dataset? What are the units of the band “NO2_column_number_density”? What is the spatial resolution of this band? What is its maximum value? 1.4.1 Answers 2018-07-10 - Present, so ~1.5 months shy of 2 years 2018-06-28 - Present, so only ~ 0.5 month longer mol/m2 3.5 x 7 km2 0.0096 1.5 Launch Code Editor with dataset Scroll to the bottom. You should see the following snippet of JavaScript code: var collection = ee.ImageCollection(&#39;COPERNICUS/S5P/NRTI/L3_NO2&#39;) .select(&#39;NO2_column_number_density&#39;) .filterDate(&#39;2019-06-01&#39;, &#39;2019-06-06&#39;); var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;] }; Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.setCenter(65.27, 24.11, 4); Click the button at the bottom to launch the Code Editor with this dataset loading JavaScript code: Open in Code Editor 1.6 Run the initial dataset script You should now be seeing the Code Editor. Here are some labels for the user interface to orient yourself: Credit: GeoHackWeek - Google Earth Engine: Code Editor Click Run to run the script. Voila! You should see tiles of the satellite data layer appear in the lower Map pane: Be sure to try out the zoom (+/-) and pan (hand icon) to move around. 1.7 Modify the script so satellite layer is semi-transparent Decrease the “opacity” (Search for this word under Docs tab; see documentation for Map.addLayer then ee.data.getMapId) in and add the opacity parameter with a value of 0.5 to the band_viz definition like so (don’t forget the extra comma): var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;], opacity: 0.5 }; Save the file and Run. I am choosing to save this file as “no2” under the “meds-demo” repository in the Script manager pane (upper left). 1.8 Add city polygon asset Use the Asset Manager (Assets tab in upper left) to now add your study area by Uploading a NEW Shape file. Drag city_Delhi.India.zip in your file system to the SELECT button. Click UPLOAD to start upload. Now click the Tasks tab (upper right) to see that this process is initiated, but not yet complete (spinny gear on right). It should complete within a minute. You might need to refresh your browser for the asset to appear. Hover over the newly added asset city_Delhi-India in your Assets pane and click on the blue right arrow to import it into your script. For more on this topic, see Managing Assets. 1.9 Add to city to map Notice how it brings in the asset as a default variable named table. I suggest changing the name of this variable to something friendlier like city_ply, short for city polygon. Now center the map on this polygon (vs Map.setCenter(65.27, 24.11, 4);) and add it as a layer to be drawn on the map: Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.centerObject(city_ply); Map.addLayer(city_ply, {color: &#39;black&#39;}, &#39;Delhi&#39;); For more, see: Geometry Visualization and Information 1.10 Create time series chart Next let’s generate a Time Series Chart, which is available as user interface (ui) that we can print (vs embedding in a dedicate app or user interface – for another day). Its two required parameters are for an image collection and a region (find parameters in the Docs). The third parameter is for the reducer, which is the function that reduces the many pixels into a single value. In our case we want to take the average so we’ll use the ee.Reducer.mean() function. print(ui.Chart.image.series( collection, city_ply, ee.Reducer.mean())); Save and Run. Yay! You should see a time series chart in the Console: 1.11 Expand dates to all available But that was only for the default week of ‘2019-06-01’ to ‘2019-06-06’ and we know this dataset is available for a longer period. Let’s update .filterDate() to use the entire range of available data in line 3: .filterDate(&#39;2018-07-10&#39;, &#39;2020-05-26&#39;); Save and Run again. It should take a bit longer to process, since it’s now extracting almost 2 years worth of data in the city polygon. Once the plot shows up in the Console again, click the upper right arrow to pop it into its own dedicated window. 1.12 Download CSV In the popped out window of the time series chart, download the comma-seperated value (CSV) text file by clicking the Download CSV button. I save my file into data/no2_delhi-india_ee-chart.csv of this Github repository. Notice in the downloaded CSV that all the values are 0, 0.001 or missing. Apparently the download functionality rounds values to the nearest 0.001. So to get the kind of precision we need let’s multiply each image in the collection by 1000 (going from moles to micromoles of NO2 per m2) using the map() function, being sure to reapply the datetime stamp for the image: var collection = ee.ImageCollection(&#39;COPERNICUS/S5P/NRTI/L3_NO2&#39;) .select(&#39;NO2_column_number_density&#39;) .filterDate(&#39;2018-07-10&#39;, &#39;2020-05-26&#39;) .map(function(img){ return img.multiply(1000) .set(&#39;system:time_start&#39;, img.get(&#39;system:time_start&#39;)) }); var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;], opacity: 0.5 }; Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.addLayer(city_ply, {color: &#39;black&#39;}, &#39;Delhi, India&#39;); Map.centerObject(city_ply); print(ui.Chart.image.series( collection, city_ply, ee.Reducer.mean())); Hit Run and download the CSV again from the chart. 1.13 Your Turn Pick another city and start from the top. Generate a polygon zip, upload as an asset and generate the time series. Conclusions It is similarly creating an average raster for the entire time domain of this dataset in the Map pane. Try zooming out, realizing that is happening globally. Now that’s big data! In a tiny amount of time, with minor effort. Further Resources Google Earth Engine GEE Lessons - GeoHackWeek: Software Carpentry style rgee: R package for GEE; still in early development (Gorelick et al. 2017) References "],
["sentiment.html", "Lesson 2 Sentiment Objectives Prerequisites 2.1 Load R packages 2.2 Setup Twitter 2.3 Define queries for air quality and clean energy 2.4 Define spatial and temporal extent 2.5 Get tweets, iterating over queries &amp; months 2.6 Classify sentiment of tweets over time, using lexicon 2.7 Twitter Sentiment140 dataset 2.8 Train TensorFlow model with Sentiment140 2.9 Classify sentiment of tweets over time, using TensorFlow", " Lesson 2 Sentiment Objectives Question How has sentiment around air quality and clean energy changed since air became cleaner? Background. In anticipation of the 2008 Summer Olympics in Beijing the Chinese government imposed strict rules to clean up the air. The people got used to better air and now Beijing is consistently improved. Will this hiatus to bad air be temporary or incite a push towards cleaner energy. For a compelling summary, check out the podcast The Natural Experiment - 99% Invisible. Technical Motivation Sentiment can be evaluated as either positive or negative. This binary classification is the most basic response for machine learning, thus a good example for a lesson on machine learning. Text however can have many complicated forms, such as negating terms (eg “happy” vs “not happy”) and sarcasm (“this air quality is so great! as if.”), which makes it a good candidate for deep learning. Approach Get tweets about air quality and clean energy throughout the period of the satellite data availability for NO2. Lookup tweeted words with a lexicon labelling positive or negative, and tally the score. Bren’s own Casey O’Hara &amp; Jessica Couture already explained this approach well in their Eco-Data-Science workshop Text analysis with R (2019-02-05). Use a Twitter dataset pre-labeled with sentiment to train a TensorFlow text classification model. Compare accuracy of TensorFlow model with the lexicon results. Classify twitter sentiment with the TensorFlow model over time for air quality and clean energy.. Prerequisites A Twitter developer account is required to download tweets and access the https://developer.twitter.com/en/dashboard. You’ll need to apply via the Twitter developer signup. I recieved an email for clarification and had the account approved and running by the end of the day. 2.1 Load R packages The librarian R package will load packages, installing them if needed, including from Github. # load libraries ---- # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) pkgs &lt;- c( # utility &quot;here&quot;,&quot;glue&quot;, &quot;readr&quot;,&quot;dplyr&quot;,&quot;tidyr&quot;,&quot;purrr&quot;,&quot;scales&quot;, &quot;lubridate&quot;,&quot;stringr&quot;, # plotting &quot;ggplot2&quot;,&quot;plotly&quot;, # spatial &quot;sf&quot;, # text &quot;rtweet&quot;,&quot;tidytext&quot;,&quot;textdata&quot;, # tensorflow &quot;tensorflow&quot;,&quot;keras&quot;) shelf(pkgs) 2.2 Setup Twitter Start off using the excellent rtweet R package to get twitter data. You’ll need to setup an access token though after applying for a Twitter developer account. See Obtaining and using access tokens • rtweet to: Create a Twitter App Authorization methods Access token/secret method Authorization in future R sessions library(rtweet) get_token() The free Twitter developer API access only goes back 1 week. I upgraded to Premium ($99/month for 50 requests per month) for the Full Archive search going back to Twitter beginnings in 2006. Unfortunately I couldn’t get the rtweet::search_fullarchive() to return any results. I had to write a custom function get_tweets_premium() and added it to functions.R. I called the app “clean-air-covid” and developer environment label “research” as well as saved the API key and secret in my home folder under ~/private. IMPORTANT: Keep the Twitter API ‘key’ and ‘secret’ secret. That means that if you are pushing these files to a Github public repository, then you’ll need to save these outside the repo, which is why I chose ~/private. For more, see: Dev environment — Twitter Developers Search Tweets - Twitter Developers # Twitter API parameters env_label &lt;- &quot;research&quot; appname &lt;- &quot;clean-air-covid&quot; key &lt;- readLines(&quot;~/private/twitter_clean-air-covid_api-key.txt&quot;) secret &lt;- readLines(&quot;~/private/twitter_clean-air-covid_api-key-secret.txt&quot;) 2.3 Define queries for air quality and clean energy These terms were pulled from the following references after a little Google Scholar searching: air quality (Gurajala, Dhaniyala, and Matthews 2019) clean energy (Sluban et al. 2015) query_air &lt;- &quot;#AIRPOLLUTION OR #AIRQUALITY OR #CLEANAIR OR #HAZE OR #OZONE OR #PARTICLES OR #PARTICULATES OR #PM25 OR #PM2.5 OR #PM10 OR #POLLUTION OR #SMOG OR #EMISSIONS&quot; query_energy &lt;- &quot;#green OR #cleanenergy OR #renewable OR #renewableenergy OR #sustainable OR #sustainability OR #solar OR #wind OR #solarpower OR #windpower OR #photovoltaic OR #biomass OR #biofuel OR #biofuels&quot; queries &lt;- list(air = query_air, energy = query_energy) Note: Twitter searches are not case-sensitive. 2.4 Define spatial and temporal extent Let’s get the spatial and temporal extents from our previous satellite analysis: Spatial. Let’s get the centroid from the city’s polygon boundary (data/city_Delhi-India.geojson) and use the maximum radius (25 miles) for Twitter searches. Temporal. Let’s get tweets for the same timespan as the downloaded CSV of NO2 satellite data (data/no2_delhi-india_ee-chart.csv). library(sf) library(lubridate) # variables &amp; files loc_name &lt;- &quot;delhi&quot; city_geo &lt;- here(glue(&quot;data/city_{loc_name}.geojson&quot;)) no2_csv &lt;- here(glue(&quot;data/no2_{loc_name}_ee-chart.csv&quot;)) # spatial loc_lonlat &lt;- read_sf(city_geo) %&gt;% st_drop_geometry() %&gt;% select(lon, lat) %&gt;% as.numeric() loc_radius_mi &lt;- 25 # max radius for tweet point_radius search # temporal date_range &lt;- read_csv(no2_csv) %&gt;% mutate( date = mdy(`system:time_start`)) %&gt;% pull(date) %&gt;% range() date_months &lt;- seq(floor_date(date_range[1], unit = &quot;month&quot;), date_range[2], by = &quot;months&quot;) range(date_months) ## [1] &quot;2018-07-01&quot; &quot;2020-05-01&quot; 2.5 Get tweets, iterating over queries &amp; months Let’s now iterate over each query (query_air and query_energy) and each month over our temporal period of interest (date_range). # define: get_tweets_premium(), clean_tweet() source(here(&quot;functions.R&quot;)) tweets_all_csv &lt;- here(glue(&#39;data/tweets_{loc_name}.csv&#39;)) n_queries &lt;- length(names(queries)) n_months &lt;- length(date_months) if (!file.exists(tweets_all_csv)){ dir_tweets &lt;- here(&quot;data/tweets&quot;) dir.create(dir_tweets, showWarnings = F) # iterate over queries for (i_query in seq_along(names(queries))){ # i_query = 1 query_name &lt;- names(queries)[i_query] query &lt;- queries[[query_name]] message(glue(&quot;query {i_query}/{n_queries} {query_name}: {query}&quot;)) # iterate over temporal range by month for (i_month in seq_along(date_months)){ # i_month = 23 date_beg &lt;- date_months[i_month] date_end &lt;- date_beg + months(1) ym_beg &lt;- format(date_beg, &quot;%Y-%m&quot;) tweets_csv &lt;- here(glue(&#39;{dir_tweets}/tweets_{loc_name}_{query_name}_{ym_beg}.csv&#39;)) if (file.exists(tweets_csv)) next message(glue(&quot; month {i_month}/{n_months} {date_beg}: {basename(tweets_csv)}&quot;)) tbl &lt;- get_tweets_premium( env_label, appname, key, secret, query, date_beg, date_end, loc_lonlat, loc_radius_mi, tweets_csv) } } # read tweets into table regexp &lt;- &quot;tweets_([a-z]+)_([a-z]+)_([0-9-]+).csv&quot; tweets &lt;- tibble( csv = list.files(here(&quot;data/tweets&quot;), full.names = T)) %&gt;% mutate( loc_name = str_replace(basename(csv), regexp, &quot;\\\\1&quot;), query_name = str_replace(basename(csv), regexp, &quot;\\\\2&quot;), ym_str = str_replace(basename(csv), regexp, &quot;\\\\3&quot;), ym_date = as.Date(glue(&quot;{ym_str}-01&quot;)), data = map(csv, read_csv), n_tweets = map_int(data, nrow)) %&gt;% unnest(data) write_csv(tweets, tweets_all_csv) } else { tweets &lt;- read_csv(tweets_all_csv) } g &lt;- tweets %&gt;% group_by(query_name, ym_date, n_tweets) %&gt;% summarize() %&gt;% rename(query = query_name) %&gt;% ggplot( aes(x = ym_date, y = n_tweets, color = query)) + geom_line() ggplotly(g) 2.6 Classify sentiment of tweets over time, using lexicon Now let’s borrow from Text analysis with R (2019-02-05), created by Bren’s own Casey O’Hara &amp; Jessica Couture as an Eco-Data-Science workshop, to use get a sentiment score based on a lexicon, or dictionary of words with assigned sentiment assigned. lex_bing &lt;- get_sentiments(&#39;bing&#39;) # clean out non-ascii, twitter handles, and urls tweets &lt;- tweets %&gt;% mutate( text_clean = clean_tweet(text)) # tweets to words words &lt;- tweets %&gt;% select(id, created_at, user_name, text_clean) %&gt;% unnest_tokens(output = word, input = text_clean, token = &quot;words&quot;) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% left_join(lex_bing, by = &quot;word&quot;) %&gt;% left_join( tribble( ~sentiment, ~lex_score, &quot;positive&quot;, 1, &quot;negative&quot;, -1), by = &quot;sentiment&quot;) # tally lex_score per tweet tweets &lt;- tweets %&gt;% left_join( words %&gt;% group_by(id) %&gt;% summarize( lex_score = mean(lex_score, na.rm = T)), by = &quot;id&quot;) # tally lex_score per month months &lt;- tweets %&gt;% group_by(query_name, ym_date) %&gt;% summarize(lex_score = mean(lex_score, na.rm = T)) g &lt;- months %&gt;% rename(query = query_name) %&gt;% ggplot( aes(x = ym_date, y = lex_score, color = query)) + geom_line() ggplotly(g) 2.7 Twitter Sentiment140 dataset For Academics - Sentiment140 - A Twitter Sentiment Analysis Tool # web link to download dataset and folder in which to store s140_url &lt;- &quot;http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip&quot; s140_dir &lt;- here(&quot;data/sentiment140&quot;) # download and unzip sentiment140 data if (length(list.files(s140_dir)) &lt; 2){ s140_zip &lt;- glue(&quot;{s140_dir}/{basename(s140_url)}&quot;) dir.create(s140_dir, showWarnings = F) download.file(s140_url, s140_zip) unzip(s140_zip, exdir = s140_dir) unlink(s140_zip) } # get training and test files test_csv &lt;- list.files(s140_dir, &quot;^test.*&quot;, full.names = T) # read in testing dataset # polarity: convert negative 0 -&gt; -1, neutral 2 -&gt; 0, positive 4 -&gt; 1 s140_cols &lt;- c(&quot;polarity&quot;, &quot;id&quot;, &quot;created_at&quot;, &quot;query&quot;, &quot;screen_name&quot;, &quot;text&quot;) test &lt;- read_csv(test_csv, col_names = s140_cols) %&gt;% mutate( polarity = recode(polarity, `0` = -1, `2` = 0, `4` = 1), text_clean = clean_tweet(text)) # test tweets to words test_words &lt;- test %&gt;% select(id, created_at, screen_name, text_clean) %&gt;% unnest_tokens(output = word, input = text_clean, token = &quot;words&quot;) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% left_join(lex_bing, by = &quot;word&quot;) %&gt;% left_join( tribble( ~sentiment, ~lex_score, &quot;positive&quot;, 1, &quot;negative&quot;, -1), by = &quot;sentiment&quot;) # tally lex_score per tweet test &lt;- test %&gt;% left_join( test_words %&gt;% group_by(id) %&gt;% summarize( lex_score = mean(lex_score, na.rm = T)), by = &quot;id&quot;) hist(test$lex_score) # performance test &lt;- test %&gt;% mutate( lex_accurate = case_when( polarity == -1 &amp; lex_score &lt; 0 ~ T, polarity == 0 &amp; lex_score == 0 ~ T, polarity == 1 &amp; lex_score &gt; 0 ~ T, T ~ F)) #select(test, polarity, lex_score, lex_accurate, text_clean) sum(test$lex_accurate) / nrow(test) ## [1] 0.3975904 2.8 Train TensorFlow model with Sentiment140 Now we’ll train a deep learning model on the Sentiment140 dataset before using to classify the tweets. This will follow the Text Classification | TensorFlow for R example, except we’ll be using the Sentiment140 dataset from Twitter instead of the IMDB movie review dataset. The text of the tweets must be converted to tensors before fed into the neural network. First, we create a dictionary and represent each of the 10,000 most common words by an integer. In this case, every tweet will be represented by a sequence of integers. NOTE: I am unable to knit this Rmarkdown file to html when running tensorflow non-interactively, so the following chunks were run manually in RStudio and set to eval=F to render this web page. Outputs have been roughly replicated below R chunks. train_csv &lt;- list.files(s140_dir, &quot;^train.*&quot;, full.names = T) # load training dataset only if needed since large train &lt;- read_csv(train_csv, col_names = s140_cols) %&gt;% mutate( polarity = recode(polarity, `0` = -1, `2` = 0, `4` = 1), text_clean = clean_tweet(text)) # examine labels in training dataset train %&gt;% count(polarity) # A tibble: 2 x 2 polarity n &lt;dbl&gt; &lt;int&gt; 1 -1 800000 2 1 800000 # create padded arrays num_words &lt;- 10000 max_length &lt;- 50 text_vectorization &lt;- layer_text_vectorization( max_tokens = num_words, output_sequence_length = max_length) # adapt the text vectorization layer # to learn about unique words in our dataset # and assign an integer value for each one text_vectorization %&gt;% adapt(train$text_clean) # check out the first 20 words get_vocabulary(text_vectorization) %&gt;% head(20) usr i to the a my and you is it for in of im on me so have that but # check out how the text vectorization layer transforms it’s inputs text_vectorization(matrix(train$text_clean[1], ncol = 1)) tf.Tensor( [[ 2 41 464 105 6 1217 9 3496 51 872 1 14 1875 33 4 43 11 449 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 50), dtype=int64) # build the model by stacking layers input &lt;- layer_input(shape = c(1), dtype = &quot;string&quot;) # input data consists of an array of word-indices # labels to predict are either -1 or 1 output &lt;- input %&gt;% text_vectorization() %&gt;% layer_embedding(input_dim = num_words + 1, output_dim = 16) %&gt;% layer_global_average_pooling_1d() %&gt;% layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) model &lt;- keras_model(input, output) # configure the model to use an optimizer and a loss function model %&gt;% compile( optimizer = &#39;adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = list(&#39;accuracy&#39;)) summary(model) Model: &quot;model&quot; ___________________________________________________________ Layer (type) Output Shape Param # =========================================================== input_1 (InputLayer) [(None, 1)] 0 ___________________________________________________________ text_vectorization (TextV (None, 50) 0 ___________________________________________________________ embedding (Embedding) (None, 50, 16) 160016 ___________________________________________________________ global_average_pooling1d (None, 16) 0 ___________________________________________________________ dense (Dense) (None, 16) 272 ___________________________________________________________ dropout (Dropout) (None, 16) 0 ___________________________________________________________ dense_1 (Dense) (None, 1) 17 =========================================================== Total params: 160,305 Trainable params: 160,305 Non-trainable params: 0 ___________________________________________________________ # train the model history &lt;- model %&gt;% fit( train$text_clean, as.numeric(train$polarity == 1), epochs = 10, batch_size = 512, validation_split = 0.2, verbose=2) RStudio Console output during model fitting: Epoch 1/10 2500/2500 - 12s - loss: 0.5038 - accuracy: 0.7653 2500/2500 - 14s - loss: 0.5038 - accuracy: 0.7653 - val_loss: 0.6292 - val_accuracy: 0.6683 Epoch 2/10 2500/2500 - 12s - loss: 0.4460 - accuracy: 0.8008 2500/2500 - 14s - loss: 0.4460 - accuracy: 0.8008 - val_loss: 0.6068 - val_accuracy: 0.6683 Epoch 3/10 2500/2500 - 12s - loss: 0.4362 - accuracy: 0.8052 2500/2500 - 13s - loss: 0.4362 - accuracy: 0.8052 - val_loss: 0.5989 - val_accuracy: 0.6673 Epoch 4/10 2500/2500 - 12s - loss: 0.4331 - accuracy: 0.8052 2500/2500 - 14s - loss: 0.4331 - accuracy: 0.8052 - val_loss: 0.6402 - val_accuracy: 0.6384 Epoch 5/10 2500/2500 - 12s - loss: 0.4316 - accuracy: 0.8057 2500/2500 - 14s - loss: 0.4316 - accuracy: 0.8057 - val_loss: 0.6075 - val_accuracy: 0.6595 Epoch 6/10 2500/2500 - 12s - loss: 0.4300 - accuracy: 0.8055 2500/2500 - 14s - loss: 0.4300 - accuracy: 0.8055 - val_loss: 0.6324 - val_accuracy: 0.6413 Epoch 7/10 2500/2500 - 12s - loss: 0.4288 - accuracy: 0.8061 2500/2500 - 14s - loss: 0.4288 - accuracy: 0.8061 - val_loss: 0.6453 - val_accuracy: 0.6341 Epoch 8/10 2500/2500 - 12s - loss: 0.4279 - accuracy: 0.8063 2500/2500 - 13s - loss: 0.4279 - accuracy: 0.8063 - val_loss: 0.5983 - val_accuracy: 0.6701 Epoch 9/10 2500/2500 - 12s - loss: 0.4273 - accuracy: 0.8071 2500/2500 - 14s - loss: 0.4273 - accuracy: 0.8071 - val_loss: 0.5900 - val_accuracy: 0.6770 Epoch 10/10 2500/2500 - 12s - loss: 0.4260 - accuracy: 0.8070 2500/2500 - 14s - loss: 0.4260 - accuracy: 0.8070 - val_loss: 0.6074 - val_accuracy: 0.6700 RStudio Viewer output during model fitting: # model accuracy and loss over iterations plot(history) # evaluate the model tf_score &lt;- predict(model, test$text_clean)[,1] %&gt;% rescale(c(-1, 1)) # performance test &lt;- test %&gt;% mutate( tf_score = tf_score, tf_accurate = case_when( polarity == -1 &amp; tf_score &lt; -0.1 ~ T, polarity == 0 &amp; tf_score &gt; -0.1 &amp; tf_score &lt; 0.1 ~ T, polarity == 1 &amp; tf_score &gt; 0.1 ~ T, T ~ F)) #select(test, polarity, tf_score, tf_accurate, text_clean) sum(test$tf_accurate) / nrow(test) [1] 0.5763052 2.9 Classify sentiment of tweets over time, using TensorFlow Now that we’ve trained the TensorFlow model, we can classify the tweets using this model. sentiments_csv &lt;- here(glue(&quot;data/sentiments_{loc_name}.csv&quot;)) if (!file.exists(sentiments_csv)){ # predict with model tweets &lt;- tweets %&gt;% mutate( tf_score = predict(model, tweets$text_clean)[,1] %&gt;% rescale(c(-1, 1))) # tally score per month months &lt;- tweets %&gt;% group_by(query_name, ym_date) %&gt;% summarize(tf_score = mean(tf_score, na.rm = T)) # save monthly results write_csv(months, sentiments_csv) } months &lt;- read_csv(sentiments_csv) g &lt;- months %&gt;% rename(query = query_name) %&gt;% ggplot( aes(x = ym_date, y = tf_score, color = query)) + geom_line() ggplotly(g) References "],
["integration.html", "Lesson 3 Integration 3.1 Time series plot of satellite &amp; sentiment metrics 3.2 Box plot of satellite &amp; sentiment metrics before/after lockdown References", " Lesson 3 Integration 3.1 Time series plot of satellite &amp; sentiment metrics Let’s now integrate data from satellite and sentiment into the same monthly time series plot, first rescaling all metrics -1 to 1 for comparison. library(here) library(glue) library(readr) library(dplyr) library(lubridate) library(scales) library(ggplot2) library(plotly) # paths loc_name &lt;- &quot;delhi&quot; satellite_csv &lt;- here(glue(&quot;data/no2_{loc_name}_ee-chart.csv&quot;)) sentiments_csv &lt;- here(glue(&quot;data/sentiments_{loc_name}.csv&quot;)) metrics_csv &lt;- here(glue(&quot;data/metrics_{loc_name}.csv&quot;)) if (!file.exists(metrics_csv)){ satellite &lt;- read_csv(satellite_csv) %&gt;% transmute( date = mdy(`system:time_start`), date_ym = floor_date(date, unit = &quot;month&quot;), no2 = NO2_column_number_density) %&gt;% filter(!is.na(no2)) %&gt;% group_by(date_ym) %&gt;% summarize( no2 = mean(no2, na.rm=T)) %&gt;% mutate( metric = &quot;NO2&quot;, value = rescale(no2, to = c(-1, 1))) %&gt;% select(metric, date_ym, value) sentiments &lt;- read_csv(sentiments_csv) %&gt;% rename(date_ym = ym_date) %&gt;% mutate( metric = glue(&quot;{query_name}&quot;), value = rescale(tf_score, to = c(-1, 1))) %&gt;% select(metric, date_ym, value) bind_rows( satellite, sentiments) %&gt;% write_csv(metrics_csv) } metrics &lt;- read_csv(metrics_csv) g &lt;- metrics %&gt;% ggplot( aes(x = date_ym, y = value, color = metric)) + geom_line() ggplotly(g) Interesting! The air quality sentiment (red line) does seem to be inversely correlated with nitrogen dioxide (NO2; blue line). The sentiment for clean energy (green line) seems to correlate with satellite NO2 until 2019-03 then remains relatively steady. 3.2 Box plot of satellite &amp; sentiment metrics before/after lockdown Let’s compare averages before and after lockdown March 24, 2020. date_lockdown &lt;- as.Date(&quot;2020-03-24&quot;) metrics_ba &lt;- bind_rows( metrics %&gt;% filter(date_ym &lt;= date_lockdown) %&gt;% mutate( period = &quot;before&quot;), metrics %&gt;% filter(date_ym &gt; date_lockdown) %&gt;% mutate( period = &quot;after&quot;)) %&gt;% mutate( period = factor(period, c(&quot;before&quot;, &quot;after&quot;), ordered = T)) # View(metrics_ba) metrics_ba %&gt;% ggplot() + geom_boxplot(aes(x = metric, y = value, fill = metric)) + facet_wrap(~period) The sentiments do not look significantly different before and after lockdown, especially relative to the dramatically different NO2 (blue). References "]
]
