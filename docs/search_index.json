[
["index.html", "Clean Air in the Time of COVID Use Cases for Satellite &amp; Sentiment Analysis Overview Motivation Prerequisites", " Clean Air in the Time of COVID Use Cases for Satellite &amp; Sentiment Analysis Ben Best, PhD 2020-05-28 Overview These materials are for a 45 minute teaching demonstration on 2020-05-26 oriented towards students in the new Masters in Environmental Data Science at UCSB. Motivation This is an exciting time for the emerging field of Environmental Data Science. Environmental problems are increasingly complex and require advanced technical skills to translate the ever expanding flow of data into actionable insights. Technologies The two specific technologies featured in this teaching demonstration highlight some of the most promising aspects of truly : Google Earth Engine leverages the massive storage and computational capabilities of the Google Cloud to analyze petabytes of the publicly available satellite data. For instance, global climatologies averaging across 40 years of temperature can be calculated and mapped in seconds. This is a truly big data platform! TensorFlow is the machine learning software made open-source by Google. It is the most commonly used software for audio, text and image analysis. More specifically tensorflow allows construction of convolutional neural networks, which represent a layering of nodes to enable complex pattern matching. These deep learning models have been popularized by their ability to beat the best human at the most complex game of Go, self-drive vehicles, Netflix recommendations, Spotify playlists and respond to your beck and call through Alexa’s voice commands. Motivating Use Case For the first time in decades, Mount Everest was visible from Kathmandu due to improved air quality. — Tom Patterson (@MtnMapper) May 21, 2020 The positive effect of the Lockdown in India’s environment can be seen in New Delhi.This picture depicts the air quality of New Delhi before &amp; after Lockdown.Source - https://t.co/VoG0UbpFzE — GurumaujSatsangi (@GurumaujS) April 23, 2020 Prerequisites See the Setup for required software. For this demonstration to make the most sense, it’s preferable that you’re familiar with: R GIS "],
["setup.html", "Setup 0.1 Apply for accounts 0.2 Install software 0.3 Launch RStudio 0.4 Fork and Clone the Github Repository 0.5 Install R Packages 0.6 Create Rmarkdown file", " Setup 0.1 Apply for accounts 0.1.1 Google Earth Engine (GEE) account Please visit: Google Earth Engine signup You may need to log out and back into your web browser as the preferred Google account to request permissions. This approval process can take days to weeks unfortunately. 0.1.2 Twitter developer account Please visit: Twitter developer signup I recieved an email for clarification and had the account approved and running by the end of the day. 0.2 Install software This workshop will require the following software installed on your machine: R RStudio Please download the appropriate stable release for your operating system. 0.3 Launch RStudio RStudio is an integrated development environment (IDE) for editing text in the code editor, running commands in the R console, viewing defined objects in the workspace and past commands in the history, and viewing plots, files and help. Here’s a layout of the panes in RStudio, each of which can have several tabs for alternate functionality: Check out the Help &gt; Cheatsheets &gt; RStudio IDE Cheat Sheet. 0.4 Fork and Clone the Github Repository Please visit https://github.com/bbest/meds-demo, signed into Github. Then into your own writable user space. Here is the Github repository of the source files for this demonstration: https://github.com/bbest/meds-demo You are encouraged to fork the repo in Github and clone it in RStudio. Then you can follow along in RStudio by evaluating the chunks of R code, while referencing the website: http://benbestphd.com/meds-demo 0.5 Install R Packages Here’s a bit of code to install packages that we’ll use throughout the workshop. Please copy and paste this code into your console. # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) # load packages pkgs &lt;- c( # general &quot;tidyverse&quot;,&quot;jsonlite&quot;,&quot;glue&quot;,&quot;here&quot;,&quot;units&quot;, # satellite &quot;sf&quot;, # sentiment &quot;rtweet&quot;,&quot;tidytext&quot;) shelf(pkgs) # report on versions of software &amp; packages sessionInfo() ## R version 3.5.2 (2018-12-20) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidytext_0.2.2 rtweet_0.7.0 sf_0.8-0 units_0.6-6 ## [5] here_0.1 glue_1.4.1 jsonlite_1.6.1 forcats_0.4.0 ## [9] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [13] tidyr_1.1.0 tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## [17] librarian_1.7.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.4.6 lubridate_1.7.8 lattice_0.20-38 class_7.3-15 ## [5] assertthat_0.2.1 rprojroot_1.3-2 digest_0.6.25 R6_2.4.1 ## [9] cellranger_1.1.0 backports_1.1.7 reprex_0.3.0 evaluate_0.14 ## [13] e1071_1.7-3 httr_1.4.1 pillar_1.4.4 rlang_0.4.6 ## [17] readxl_1.3.1 rstudioapi_0.11 Matrix_1.2-18 rmarkdown_2.1 ## [21] munsell_0.5.0 broom_0.5.5 janeaustenr_0.1.5 compiler_3.5.2 ## [25] modelr_0.1.5 xfun_0.14 pkgconfig_2.0.3 htmltools_0.4.0 ## [29] tidyselect_1.1.0 bookdown_0.16 fansi_0.4.1 crayon_1.3.4 ## [33] dbplyr_1.4.2 withr_2.2.0 SnowballC_0.6.0 grid_3.5.2 ## [37] nlme_3.1-142 gtable_0.3.0 lifecycle_0.2.0 DBI_1.1.0 ## [41] magrittr_1.5 tokenizers_0.2.1 scales_1.1.1 KernSmooth_2.23-16 ## [45] cli_2.0.2 stringi_1.4.6 fs_1.3.1 xml2_1.2.2 ## [49] ellipsis_0.3.1 generics_0.0.2 vctrs_0.3.0 tools_3.5.2 ## [53] hms_0.5.3 yaml_2.2.1 colorspace_1.4-1 classInt_0.4-3 ## [57] rvest_0.3.5 knitr_1.28 haven_2.2.0 0.6 Create Rmarkdown file Rmarkdown is a dynamic document format that allows you to knit chunks of R code with formatted text (aka markdown). We recommend that you use this format for keeping a reproducible research document as you work through the lessons To get started, File &gt; New File &gt; Rmarkdown… and go with default HTML document and give any title you like (default “Untitled” or “test” or “First Rmd” is fine). Check out the Help &gt; Markdown Quick Reference and Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet. Here’s a 1 minute video on the awesomeness of Rmarkdown: "],
["satellite.html", "Lesson 1 Satellite Objectives Prerequisites 1.1 Get city boundary 1.2 Browse datasets, tag no2 1.3 View dataset info 1.4 Questions 1.5 Launch Code Editor with dataset 1.6 Run the initial dataset script 1.7 Modify the script so satellite layer is semi-transparent 1.8 Add city polygon asset 1.9 Add to city to map 1.10 Create time series chart 1.11 Expand dates to all available 1.12 Download CSV Conclusions 1.13 Your Turn Further Resources", " Lesson 1 Satellite Objectives Question How have emissions related to air quality changed since COVID-19 lockdowns were put in place? Background. Nitrogen dioxide (NO2) is produced from burning of fossil fuel, especialy from motor vehicles, and contributes to the formation of other air pollutants, including ozone, particulate mattter and acid rain. Study area: Delhi, India We’ll use Delhi, India as our initial city study area. Prime Minister Modhi issued a nationwide lockdown on 24 March, 2020. News: Air pollution falls by unprecedented levels in major global cities during coronavirus lockdowns - CNN India: air pollution in the north has hit a 20-year low, NASA report says - CNN Fact Check: Is The COVID-19 Lockdown Decreasing Delhi Air Pollution? - Smart Air Filters Learning outcomes The student will learn how to: Browse Google Earth Engine’s data catalogue Examine basic information about the dataset Map datasets in the Code Editor Upload assets, such as a polygon Adjust time period for averaging Generate a time series chart for satellite data extracted to a region Download data as a text file (csv) Prerequisites A Google Earth Engine account that is associated with a Google account, such as from Gmail, is required to log into https://code.earthengine.google.com. If you need a GEE account, please visit https://signup.earthengine.google.com. You may need to log out and back into your web browser as the preferred Google account to request permissions. This approval process can take days to weeks unfortunately. 1.1 Get city boundary The first step is to define our study area. I made a little R helper function city2zip() to: Fetch the administrative boundary from the Nominatim OpenStreetMap API given a city name. Extract the polygon information, convert to shapefile and zip for upload into GEE. source(&quot;functions.R&quot;) city2zip(&quot;Delhi, India&quot;) The function returns the paths to files generated. You will use this zip file to upload into GEE as an asset. 1.2 Browse datasets, tag no2 Visit https://earthengine.google.com &gt; Datasets (upper right). Be sure to explore the many datasets available here. Since we know we want Nitrogen Dioxide (NO2), click on Browse by tags and Filter by “no2”. You should see two datasets: Screenshot of GEE data catalog filtered by tag “no2” showing two datasets. 1.3 View dataset info Please click on “Sentinel-5P NRTI NO2: Near Real-Time Nitrogen Dioxide” to get the dataset view. Explore the metadata for this dataset. 1.4 Questions How many years of data are available? How does this compare with the other “Offline” dataset? Which of the bands do we want to use that is closest to the surface? What are the units of the band “NO2_column_number_density”? What is its maximum value? 1.4.1 Answers 2018-07-10 - Present, so ~1.5 months shy of 2 years 2018-06-28 - Present, so only ~ 0.5 month longer mol/m2 0.0096 1.5 Launch Code Editor with dataset Scroll to the bottom. You should see the following snippet of JavaScript code: var collection = ee.ImageCollection(&#39;COPERNICUS/S5P/NRTI/L3_NO2&#39;) .select(&#39;NO2_column_number_density&#39;) .filterDate(&#39;2019-06-01&#39;, &#39;2019-06-06&#39;); var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;] }; Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.setCenter(65.27, 24.11, 4); Click the button at the bottom to launch the Code Editor with this dataset loading JavaScript code: Open in Code Editor 1.6 Run the initial dataset script You should now be seeing the Code Editor. Here are some labels for the user interface to orient yourself: Credit: GeoHackWeek - Google Earth Engine: Code Editor Click Run to run the script. Voila! You should see tiles of the satellite data layer appear in the lower Map pane: Be sure to try out the zoom (+/-) and pan (hand icon) to move around. 1.7 Modify the script so satellite layer is semi-transparent Decrease the “opacity” (Search for this word under Docs tab; see documentation for Map.addLayer then ee.data.getMapId) in and add the opacity parameter with a value of 0.5 to the band_viz definition like so (don’t forget the extra comma): var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;], opacity: 0.5 }; Save the file and Run. I am choosing to save this file as “no2” under the “meds-demo” repository in the Script manager pane (upper left). 1.8 Add city polygon asset Use the Asset Manager (Assets tab in upper left) to now add your study area by Uploading a NEW Shape file. Drag city_Delhi.India.zip in your file system to the SELECT button. Click UPLOAD to start upload. Now click the Tasks tab (upper right) to see that this process is initiated, but not yet complete (spinny gear on right). It should complete within a minute. You might need to refresh your browser for the asset to appear. Hover over the newly added asset city_Delhi-India in your Assets pane and click on the blue right arrow to import it into your script. For more on this topic, see Managing Assets. 1.9 Add to city to map Notice how it brings in the asset as a default variable named table. I suggest changing the name of this variable to something friendlier like city_ply, short for city polygon. Now center the map on this polygon (vs Map.setCenter(65.27, 24.11, 4);) and add it as a layer to be drawn on the map: Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.centerObject(city_ply); Map.addLayer(city_ply, {color: &#39;black&#39;}, &#39;Delhi&#39;); For more, see: Geometry Visualization and Information 1.10 Create time series chart Next let’s generate a Time Series Chart, which is available as user interface (ui) that we can print (vs embedding in a dedicate app or user interface – for another day). Its two required parameters are for an image collection and a region (find parameters in the Docs). The third parameter is for the reducer, which is the function that reduces the many pixels into a single value. In our case we want to take the average so we’ll use the ee.Reducer.mean() function. print(ui.Chart.image.series( collection, city_ply, ee.Reducer.mean())); Save and Run. Yay! You should see a time series chart in the Console: 1.11 Expand dates to all available But that was only for the default week of ‘2019-06-01’ to ‘2019-06-06’ and we know this dataset is available for a longer period. Let’s update .filterDate() to use the entire range of available data in line 3: .filterDate(&#39;2018-07-10&#39;, &#39;2020-05-26&#39;); Save and Run again. It should take a bit longer to process, since it’s now extracting almost 2 years worth of data in the city polygon. Once the plot shows up in the Console again, click the upper right arrow to pop it into its own dedicated window. 1.12 Download CSV In the popped out window of the time series chart, download the comma-seperated value (CSV) text file by clicking the Download CSV button. I save my file into data/no2_gee_Delhi-India.csv of this Github repository. Conclusions It is similarly creating an average raster for the entire time domain of this dataset in the Map pane. Try zooming out, realizing that is happening globally. Now that’s big data! In a tiny amount of time, with minor effort. 1.13 Your Turn Pick another city and start from the top. Generate a polygon zip, upload as an asset and generate the time series. Further Resources Google Earth Engine GEE Lessons - GeoHackWeek: Software Carpentry style rgee: R package for GEE; still in early development (Gorelick et al. 2017) References "],
["sentiment.html", "Lesson 2 Sentiment STATUS: NOT READY Objectives Prerequisites 2.1 Setup Twitter token 2.2 Load R packages 2.3 Search Twitter 2.4 Calculate dictionary score 2.5 Twitter test dataset 2.6 Introducing TensorFlow", " Lesson 2 Sentiment STATUS: NOT READY Sorry, this teaching module is not ready for students to run through just yet. I experienced a couple issues: The free Twitter developer API access only goes back 1 week. I upgraded to Premium for the Full Archive search going back to Twitter beginnings in 2006, but couldn’t get the rtweet::search_fullarchive() to return any results. I finally got a week’s worth of results with a more manual approach, _tweet-premium.R, but then became time constrained to get the rest. I wanted to use an existing trained model, i.e. “transfer learning”, to improve accuracy and not get into the weeds of convolutional neural net model design, but had trouble getting tfhub to work. Objectives Question How has sentiment around air quality and clean energy changed since air became cleaner? Background. In anticipation of the 2008 Summer Olympics in Beijing the Chinese government imposed strict rules to clean up the air. The people got used to better air and now Beijing is consistently improved. Will this hiatus to bad air be temporary or incite a push towards cleaner energy. For a compelling summary, check out the podcast The Natural Experiment - 99% Invisible. Technical Motivation Sentiment can be evaluated as either positive or negative. This binary classification is the most basic response for machine learning, thus a good example for a lesson on machine learning. Text however can have many complicated forms, such as negating terms (eg “happy” vs “not happy”), which makes it a good candidate for deep learning. Approach Get a set of sample tweets before and after the lockdown with some of key terms. Lookup tweeted words with dictionaries labelling positive or negative, and tally the score. Bren’s own Casey O’Hara &amp; Jessica Couture already explained this approach well in their Eco-Data-Science workshop Text analysis with R (2019-02-05). Introduce TensorFlow starting with a pre-trained model. Use a sample of the Sentiment140 twitter dataset to train an existing natural language processing (NLP) model. Predict that over time. Prerequisites A Twitter developer account is required to download tweets and access the https://developer.twitter.com/en/dashboard. You’ll need to apply via the Twitter developer signup. I recieved an email for clarification and had the account approved and running by the end of the day. 2.1 Setup Twitter token We’ll use the rtweet R package to get twitter data. You’ll need to setup an access token though after applying for a Twitter API See Obtaining and using access tokens • rtweet: Creating a Twitter App Authorization methods Access token/secret method Authorization in future R sessions library(rtweet) get_token() For search_fullarchive also tried to setup Dev environment — Twitter Developers, but that didn’t seem to work. 2.2 Load R packages # load libraries ---- # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) pkgs &lt;- c( # utility &quot;here&quot;,&quot;glue&quot;,&quot;stringr&quot;,&quot;dplyr&quot;,&quot;readr&quot;,&quot;ggplot2&quot;,&quot;purrr&quot;, # airquality #&quot;ropensci/ropenaq&quot;, # spatial &quot;sf&quot;,#&quot;ggmap&quot;,&quot;mapview&quot;,&quot;leaflet&quot;, # text &quot;rtweet&quot;,&quot;tidytext&quot;,&quot;textdata&quot;, # tensorflow &quot;tensorflow&quot;,&quot;keras&quot;,&quot;tfhub&quot;,&quot;rstudio/tfds&quot;,&quot;pins&quot;) shelf(pkgs) 2.3 Search Twitter Using hashtags from (Gurajala, Dhaniyala, and Matthews 2019): #AIRPOLLUTION #AIRQUALITY #CLEANAIR #HAZE #OZONE #PARTICLES #PARTICULATES #PM25 #PM2.5 #PM10 #POLLUTION #SMOG #EMISSIONS city_geo &lt;- here(&quot;data/city_Delhi-India.geojson&quot;) now_rds &lt;- here(&quot;data/twitter_aq_delhi_now.rds&quot;) yr1_rds &lt;- here(&quot;data/twitter_aq_delhi_1yr.rds&quot;) aq_hashes &lt;- c(&quot;#AIRPOLLUTION #AIRQUALITY #CLEANAIR #HAZE #OZONE #PARTICLES #PARTICULATES #PM25 #PM2.5 #PM10 #POLLUTION #SMOG #EMISSIONS&quot;) %&gt;% str_replace_all(&quot; &quot;, &quot; OR &quot;) q_bb &lt;- read_sf(city_geo) %&gt;% st_bbox() %&gt;% glue_data( &quot;bounding_box:[{xmin} {ymin} {xmax} {ymax}]&quot;) geocode_str &lt;- read_sf(city_geo) %&gt;% glue_data( &quot;{round(lat, 4)},{round(lon, 4)},{round(r_mi*2, 2)}mi&quot;) q_geo &lt;- read_sf(city_geo) %&gt;% glue_data( &quot;point_radius:[{round(lon, 4)} {round(lat, 4)} {round(r_mi*2, 2)}mi]&quot;) if (!file.exists(now_rds)){ tbl &lt;- search_tweets( q = aq_hashes, geocode = geocode_str, n = 1000) saveRDS(tbl, now_rds) } if (!file.exists(yr1_rds)){ #if (F){ tbl_yr1 &lt;- search_fullarchive( env_name = &quot;research&quot;, fromDate = &quot;201905190000&quot;, toDate = &quot;201905260000&quot;, q = glue(&quot;({aq_hashes}) {q_geo}&quot;), n = 1000) saveRDS(tbl_yr1, yr1_rds) } 2.4 Calculate dictionary score s_b &lt;- get_sentiments(&#39;bing&#39;) # s_a &lt;- get_sentiments(&#39;afinn&#39;) # s_n &lt;- get_sentiments(&#39;nrc&#39;) tbl &lt;- readRDS(now_rds) # clean out non-ascii, twitter handles, and urls tbl &lt;- tbl %&gt;% mutate( text_clean = text %&gt;% str_replace_all(&quot;[^[:ascii:]]&quot;, &quot;_&quot;) %&gt;% tolower() %&gt;% str_replace_all(&quot;@[^ ]+&quot;, &quot;_usr_&quot;) %&gt;% str_replace_all(&quot;http[^ ]+&quot;, &quot;_url_&quot;)) # tweets to words words &lt;- tbl %&gt;% select(status_id, created_at, screen_name, text_clean) %&gt;% unnest_tokens(output = word, input = text_clean, token = &quot;words&quot;) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% left_join(s_b, by = &quot;word&quot;) %&gt;% left_join( tribble( ~sentiment, ~score, &quot;positive&quot;, 1, &quot;negative&quot;, -1), by = &quot;sentiment&quot;) # tally score per tweet tbl &lt;- tbl %&gt;% left_join( words %&gt;% group_by(status_id) %&gt;% summarize( score = mean(score, na.rm = T)), by = &quot;status_id&quot;) hist(tbl$score) mean(na.omit(tbl$score)) ## [1] -0.2354447 nrow(tbl) ## [1] 607 2.5 Twitter test dataset For Academics - Sentiment140 - A Twitter Sentiment Analysis Tool s140_csv &lt;- here(&quot;data/sentiment140_testdata.manual.2009.06.14.csv&quot;) s140 &lt;- read_csv( s140_csv, col_names = c( &quot;polarity&quot;, &quot;status_id&quot;, &quot;created_at&quot;, &quot;query&quot;, &quot;screen_name&quot;, &quot;text&quot;)) %&gt;% mutate( # convert negative 0 -&gt; -1, neutral 2 -&gt; 0, positive 4 -&gt; 1 polarity = recode(polarity, `0` = -1, `2` = 0, `4` = 1)) # clean out non-ascii, twitter handles, and urls s140 &lt;- s140 %&gt;% mutate( text_clean = text %&gt;% str_replace_all(&quot;[^[:ascii:]]&quot;, &quot;_&quot;) %&gt;% tolower() %&gt;% str_replace_all(&quot;@[^ ]+&quot;, &quot;_usr_&quot;) %&gt;% str_replace_all(&quot;http[^ ]+&quot;, &quot;_url_&quot;)) # tweets to words words &lt;- s140 %&gt;% select(status_id, created_at, screen_name, text_clean) %&gt;% unnest_tokens(output = word, input = text_clean, token = &quot;words&quot;) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% left_join(s_b, by = &quot;word&quot;) %&gt;% left_join( tribble( ~sentiment, ~score, &quot;positive&quot;, 1, &quot;negative&quot;, -1), by = &quot;sentiment&quot;) # tally score per tweet s140 &lt;- s140 %&gt;% left_join( words %&gt;% group_by(status_id) %&gt;% summarize( score = mean(score, na.rm = T)), by = &quot;status_id&quot;) hist(s140$score) nrow(s140) ## [1] 498 mean(na.omit(s140$score)) ## [1] -0.07255595 # performance s140 &lt;- s140 %&gt;% mutate( accurate_dict = case_when( polarity == -1 &amp; score &lt; 0 ~ T, polarity == 0 &amp; score == 0 ~ T, polarity == 1 &amp; score &gt; 0 ~ T, T ~ F)) select(s140, polarity, score, accurate_dict) ## # A tibble: 498 x 3 ## polarity score accurate_dict ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1 1 TRUE ## 2 1 1 TRUE ## 3 1 -1 FALSE ## 4 1 0 FALSE ## 5 1 1 TRUE ## 6 1 1 TRUE ## 7 -1 -1 TRUE ## 8 1 NaN FALSE ## 9 1 1 TRUE ## 10 1 1 TRUE ## # … with 488 more rows sum(s140$accurate_dict / nrow(s140)) ## [1] 0.3975904 2.6 Introducing TensorFlow RStudio AI Blog: tfhub: R interface to TensorFlow Hub You need to install these Python packages once: tensorflow::install_tensorflow() keras::install_keras() tfhub::install_tfhub() tfds::install_tfds() reticulate::py_config() 2.6.1 Text classification Text Classification The dataset was downloaded from Movie Reviews | Kaggle. #df &lt;- read_csv(here(&quot;data/movie_review.csv&quot;)) df &lt;- s140 %&gt;% mutate( # convert negative -1 -&gt; neg, neutral 0 -&gt; neu, positive 1 -&gt; pos tag = recode(polarity, `-1` = &quot;neg&quot;, `0` = &quot;neu&quot;, `1` = &quot;pos&quot;)) df %&gt;% count(tag) ## # A tibble: 3 x 2 ## tag n ## &lt;chr&gt; &lt;int&gt; ## 1 neg 177 ## 2 neu 139 ## 3 pos 182 df$text[1] ## [1] &quot;@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.&quot; # split our dataset into training and testing training_id &lt;- sample.int(nrow(df), size = nrow(df)*0.8) training &lt;- df[training_id,] testing &lt;- df[-training_id,] # distribution of number of words in each review? #df$text %&gt;% df$text_clean %&gt;% strsplit(&quot; &quot;) %&gt;% sapply(length) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 8.00 12.00 13.73 18.75 31.00 # create padded arrays num_words &lt;- 10000 max_length &lt;- 50 text_vectorization &lt;- layer_text_vectorization( max_tokens = num_words, output_sequence_length = max_length,) text_vectorization %&gt;% #adapt(df$text) adapt(df$text_clean) # TODO see https://github.com/tensorflow/tensorflow/pull/34529 get_vocabulary(text_vectorization) %&gt;% head(20) ## [1] &quot;the&quot; &quot;i&quot; &quot;to&quot; &quot;url&quot; &quot;a&quot; &quot;usr&quot; &quot;is&quot; &quot;and&quot; &quot;for&quot; ## [10] &quot;my&quot; &quot;at&quot; &quot;of&quot; &quot;in&quot; &quot;it&quot; &quot;with&quot; &quot;time&quot; &quot;you&quot; &quot;just&quot; ## [19] &quot;on&quot; &quot;night&quot; text_vectorization(matrix(df$text[1], ncol = 1)) ## tf.Tensor( ## [[ 1 3 1322 11 47 34 29 2 381 8 299 38 2 85 ## 8 1594 14 31 520 208 0 0 0 0 0 0 0 0 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 0 0 0 0 0 0 0 0]], shape=(1, 50), dtype=int64) input &lt;- layer_input(shape = c(1), dtype = &quot;string&quot;) output &lt;- input %&gt;% text_vectorization() %&gt;% layer_embedding(input_dim = num_words + 1, output_dim = 16) %&gt;% layer_global_average_pooling_1d() %&gt;% layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) model &lt;- keras_model(input, output) # configure the model to use an optimizer and a loss function model %&gt;% compile( optimizer = &#39;adam&#39;, loss = &#39;binary_crossentropy&#39;, metrics = list(&#39;accuracy&#39;)) # train the model history &lt;- model %&gt;% fit( training$text, as.numeric(training$tag == &quot;pos&quot;), epochs = 10, batch_size = 512, validation_split = 0.2, verbose=2) # evaluate the model results &lt;- model %&gt;% evaluate(testing$text, as.numeric(testing$tag == &quot;pos&quot;), verbose = 0) results ## loss accuracy ## 0.6820824 0.6500000 # graph accuracy and loss over iterations plot(history) results ## loss accuracy ## 0.6820824 0.6500000 2.6.2 not working on Ben’s laptop: trained model on image classification library(tfhub) library(keras) layer_mobilenet &lt;- layer_hub( handle = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4&quot;) input &lt;- layer_input(shape = c(224, 224, 3)) output &lt;- layer_mobilenet(input) model &lt;- keras_model(input, output) summary(model) The layer_hub() function above however kept throwing errors like this: Error in py_call_impl(callable, dots$args, dots$keywords) : OSError: SavedModel file does not exist at: /var/folders/2r/grqvdjfn04361tzk8mh60st40000gn/T/tfhub_modules/426589ad685896ab7954855255a52db3442cb38d/{saved_model.pbtxt|saved_model.pb} There’s a lot of Python and R communication that can get easily confused between versions. So let’s switch to a clean installation by using Docker. Install Docker if you don’t already have it. Then per rocker/tensorflow - Docker Hub run the following to get a clean RStudio instance with all the TensorFlow and Python dependencies properly installed: docker run -e PASSWORD=mu -p 8787:8787 --name ml4r rocker/ml Visit http://localhost:8787/ and enter username rstudio, password mu. Then in the Terminal of RStudio run: git clone https://github.com/bbest/meds-demo img &lt;- image_load(&quot;data/grace-hopper.jpg&quot;, target_size = c(224,224)) %&gt;% image_to_array() img &lt;- img/255 dim(img) &lt;- c(1, dim(img)) pred &lt;- predict(model, img) imagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]] "]
]
