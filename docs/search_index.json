[
["index.html", "Clean Air in the Time of COVID Use Cases for Google Earth Engine &amp; Sentiment Analysis Overview Motivation Prerequisites", " Clean Air in the Time of COVID Use Cases for Google Earth Engine &amp; Sentiment Analysis Ben Best, PhD 2020-05-25 Overview These materials are for a 45 minute teaching demonstration on 2020-05-26 oriented towards students in the new Masters in Environmental Data Science at UCSB. Motivation This is an exciting time for the emerging field of Environmental Data Science. Environmental problems are increasingly complex and require advanced technical skills to translate the ever expanding flow of data into actionable insights. Technologies The two specific technologies featured in this teaching demonstration highlight some of the most promising aspects of truly : Google Earth Engine leverages the massive storage and computational capabilities of the Google Cloud to analyze petabytes of the publicly available satellite data. For instance, global climatologies averaging across 40 years of temperature can be calculated and mapped in seconds. This is a truly big data platform! TensorFlow is the machine learning software made open-source by Google. It is the most commonly used software for audio, text and image analysis. More specifically tensorflow allows construction of convolutional neural networks, which represent a layering of nodes to enable complex pattern matching. These deep learning models have been popularized by their ability to beat the best human at the most complex game of Go, self-drive vehicles and respond to your beck and call through Alexa‚Äôs voice commands. Motivating Use Case For the first time in decades, Mount Everest was visible from Kathmandu due to improved air quality. ‚Äî Tom Patterson (@MtnMapper) May 21, 2020 The positive effect of the Lockdown in India‚Äôs environment can be seen in New Delhi.This picture depicts the air quality of New Delhi before &amp; after Lockdown.Source - https://t.co/VoG0UbpFzE ‚Äî GurumaujSatsangi (@GurumaujS) April 23, 2020 Prerequisites See the Setup for required software. For this demonstration to make the most sense, it‚Äôs preferable that you‚Äôre familiar with: R GIS "],
["setup.html", "Setup 0.1 Install software 0.2 Launch RStudio 0.3 Fork and Clone the Github Repository 0.4 Install R Packages 0.5 Create Rmarkdown file", " Setup 0.1 Install software This workshop will require the following software installed on your machine: R RStudio Please download the appropriate stable release for your operating system. 0.2 Launch RStudio RStudio is an integrated development environment (IDE) for editing text in the code editor, running commands in the R console, viewing defined objects in the workspace and past commands in the history, and viewing plots, files and help. Here‚Äôs a layout of the panes in RStudio, each of which can have several tabs for alternate functionality: Check out the Help &gt; Cheatsheets &gt; RStudio IDE Cheat Sheet. 0.3 Fork and Clone the Github Repository Please visit https://github.com/bbest/meds-demo, signed into Github. Then into your own writable user space. Here is the Github repository of the source files for this demonstration: https://github.com/bbest/meds-demo You are encouraged to fork the repo in Github and clone it in RStudio. Then you can follow along in RStudio by evaluating the chunks of R code, while referencing the website: http://benbestphd.com/meds-demo 0.4 Install R Packages Here‚Äôs a bit of code to install packages that we‚Äôll use throughout the workshop. Please copy and paste this code into your console. # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) # load packages pkgs &lt;- c( # general &quot;tidyverse&quot;,&quot;jsonlite&quot;,&quot;glue&quot;,&quot;here&quot;,&quot;units&quot;, # satellite &quot;sf&quot;, # sentiment &quot;rtweet&quot;,&quot;tidytext&quot;) shelf(pkgs) # report on versions of software &amp; packages sessionInfo() ## R version 3.5.2 (2018-12-20) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidytext_0.2.2 rtweet_0.7.0 sf_0.8-0 units_0.6-6 ## [5] here_0.1 glue_1.4.1 jsonlite_1.6.1 forcats_0.4.0 ## [9] stringr_1.4.0 dplyr_0.8.5 purrr_0.3.4 readr_1.3.1 ## [13] tidyr_1.1.0 tibble_3.0.1 ggplot2_3.3.0 tidyverse_1.3.0 ## [17] librarian_1.7.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.4.6 lubridate_1.7.8 lattice_0.20-38 class_7.3-15 ## [5] assertthat_0.2.1 rprojroot_1.3-2 digest_0.6.25 R6_2.4.1 ## [9] cellranger_1.1.0 backports_1.1.7 reprex_0.3.0 evaluate_0.14 ## [13] e1071_1.7-3 httr_1.4.1 pillar_1.4.4 rlang_0.4.6 ## [17] readxl_1.3.1 rstudioapi_0.11 Matrix_1.2-18 rmarkdown_2.1 ## [21] munsell_0.5.0 broom_0.5.5 janeaustenr_0.1.5 compiler_3.5.2 ## [25] modelr_0.1.5 xfun_0.14 pkgconfig_2.0.3 htmltools_0.4.0 ## [29] tidyselect_1.1.0 bookdown_0.16 fansi_0.4.1 crayon_1.3.4 ## [33] dbplyr_1.4.2 withr_2.2.0 SnowballC_0.6.0 grid_3.5.2 ## [37] nlme_3.1-142 gtable_0.3.0 lifecycle_0.2.0 DBI_1.1.0 ## [41] magrittr_1.5 tokenizers_0.2.1 scales_1.1.1 KernSmooth_2.23-16 ## [45] cli_2.0.2 stringi_1.4.6 fs_1.3.1 xml2_1.2.2 ## [49] ellipsis_0.3.1 generics_0.0.2 vctrs_0.3.0 tools_3.5.2 ## [53] hms_0.5.3 yaml_2.2.1 colorspace_1.4-1 classInt_0.4-3 ## [57] rvest_0.3.5 knitr_1.28 haven_2.2.0 0.5 Create Rmarkdown file Rmarkdown is a dynamic document format that allows you to knit chunks of R code with formatted text (aka markdown). We recommend that you use this format for keeping a reproducible research document as you work through the lessons To get started, File &gt; New File &gt; Rmarkdown‚Ä¶ and go with default HTML document and give any title you like (default ‚ÄúUntitled‚Äù or ‚Äútest‚Äù or ‚ÄúFirst Rmd‚Äù is fine). Check out the Help &gt; Markdown Quick Reference and Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet. Here‚Äôs a 1 minute video on the awesomeness of Rmarkdown: "],
["satellite.html", "Lesson 1 Satellite Objectives Prerequisites 1.1 Get city boundary 1.2 Browse datasets, tag no2 1.3 View dataset info 1.4 Questions 1.5 Launch Code Editor with dataset 1.6 Run the initial dataset script 1.7 Modify the script so satellite layer is semi-transparent 1.8 Add city polygon asset 1.9 Add to city to map 1.10 Create time series chart 1.11 Expand dates to all available 1.12 Download CSV Conclusions Further Resources", " Lesson 1 Satellite Objectives Question How have emissions related to air quality changed since COVID-19 lockdowns were put in place? Study area: Delhi, India We‚Äôll use Delhi, India as our initial city study area. Prime Minister Modhi issued a nationwide lockdown on 24 March, 2020. News: Air pollution falls by unprecedented levels in major global cities during coronavirus lockdowns - CNN India: air pollution in the north has hit a 20-year low, NASA report says - CNN Fact Check: Is The COVID-19 Lockdown Decreasing Delhi Air Pollution? - Smart Air Filters Learning outcomes Browse Google Earth Engine‚Äôs data catalogue Map satellite imagery dataset layer Filter by date Average values over time Upload a polygon into assets Use polygon to extract values for study area Average values in space within the polygon Generate a time series chart for satellite data Download data as a text file (csv) Prerequisites A Google Earth Engine account that is associated with a Google account, such as from Gmail, is required to log into https://code.earthengine.google.com. If you need a GEE account, please visit https://signup.earthengine.google.com. You may need to log out and back into your web browser as the preferred Google account to request permissions. This approval process can take days to weeks unfortunately. 1.1 Get city boundary The first step is to define our study area. I made a little R helper function city2zip() to: Fetch the administrative boundary from the Nominatim OpenStreetMap API given a city name. Extract the polygon information, convert to shapefile and zip for upload into GEE. source(&quot;functions.R&quot;) city2zip(&quot;Delhi, India&quot;) The function returns the paths to files generated. You will use this zip file to upload into GEE as an asset. 1.2 Browse datasets, tag no2 Visit https://earthengine.google.com &gt; Datasets (upper right). Be sure to explore the many datasets available here. Since we know we want Nitrogen Dioxide (NO2), click on Browse by tags and Filter by ‚Äúno2‚Äù. You should see two datasets: Screenshot of GEE data catalog filtered by tag ‚Äúno2‚Äù showing two datasets. 1.3 View dataset info Please click on ‚ÄúSentinel-5P NRTI NO2: Near Real-Time Nitrogen Dioxide‚Äù to get the dataset view. Explore the metadata for this dataset. 1.4 Questions How many years of data are available? How does this compare with the other ‚ÄúOffline‚Äù dataset? Which of the bands do we want to use that is closest to the surface? What are the units of the band ‚ÄúNO2_column_number_density‚Äù? What is its maximum value? 1.4.1 Answers 2018-07-10 - Present, so ~1.5 months shy of 2 years 2018-06-28 - Present, so only ~ 0.5 month longer mol/m2 0.0096 1.5 Launch Code Editor with dataset Scroll to the bottom. You should see the following snippet of JavaScript code: var collection = ee.ImageCollection(&#39;COPERNICUS/S5P/NRTI/L3_NO2&#39;) .select(&#39;NO2_column_number_density&#39;) .filterDate(&#39;2019-06-01&#39;, &#39;2019-06-06&#39;); var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;] }; Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.setCenter(65.27, 24.11, 4); Click the button at the bottom to launch the Code Editor with this dataset loading JavaScript code: Open in Code Editor 1.6 Run the initial dataset script You should now be seeing the Code Editor. Here are some labels for the user interface to orient yourself: Credit: Google Earth Engine: Code Editor Click Run to run the script. Voila! You should see tiles of the satellite data layer appear in the lower Map pane: Be sure to try out the zoom (+/-) and pan (hand icon) to move around. 1.7 Modify the script so satellite layer is semi-transparent Decrease the ‚Äúopacity‚Äù (Search for this word under Docs tab; see documentation for Map.addLayer then ee.data.getMapId) in and add the opacity parameter with a value of 0.5 to the band_viz definition like so (don‚Äôt forget the extra comma): var band_viz = { min: 0, max: 0.0002, palette: [&#39;black&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;], opacity: 0.5 }; Save the file and Run. I am choosing to save this file as ‚Äúno2‚Äù under the ‚Äúmeds-demo‚Äù repository in the Script manager pane (upper left). 1.8 Add city polygon asset Use the Asset Manager (Assets tab in upper left) to now add your study area by Uploading a NEW Shape file. Drag city_Delhi.India.zip in your file system to the SELECT button. Click UPLOAD to start upload. Now click the Tasks tab (upper right) to see that this process is initiated, but not yet complete (spinny gear on right). It should complete within a minute. You might need to refresh your browser for the asset to appear. Hover over the newly added asset city_Delhi-India in your Assets pane and click on the blue right arrow to import it into your script. For more on this topic, see Managing Assets. 1.9 Add to city to map Notice how it brings in the asset as a default variable named table. I suggest changing the name of this variable to something friendlier like city_ply, short for city polygon. Now center the map on this polygon (vs Map.setCenter(65.27, 24.11, 4);) and add it as a layer to be drawn on the map: Map.addLayer(collection.mean(), band_viz, &#39;S5P N02&#39;); Map.centerObject(city_ply); Map.addLayer(city_ply, {color: &#39;black&#39;}, &#39;Delhi&#39;); For more, see: Geometry Visualization and Information 1.10 Create time series chart Next let‚Äôs generate a Time Series Chart, which is available as user interface (ui) that we can print (vs embedding in a whole user interface ‚Äì for another day). Its two required parameters are for an image collection and a region (find parameters in the Docs). The third parameter is for the reducer, which is the function that reduces the many pixels into a single value. In our case we want to take the average so we‚Äôll use the ee.Reducer.mean() function. print(ui.Chart.image.series( collection, city_ply, ee.Reducer.mean())); Save and Run. Yay! You should see a time series chart in the Console: 1.11 Expand dates to all available But that was only for the default week of ‚Äò2019-06-01‚Äô to ‚Äò2019-06-06‚Äô and we know this dataset is available for a longer period. Let‚Äôs update .filterDate() to use the entire range of available data in line 3: .filterDate(&#39;2018-07-10&#39;, &#39;2020-05-26&#39;); Save and Run again. It should take a bit longer to process, since it‚Äôs now extracting almost 2 years worth of data in the city polygon. Once the plot shows up in the Console again, click the upper right arrow to pop it into its own dedicated window. 1.12 Download CSV In the popped out window of the time series chart, download the comma-seperated value (CSV) text file by clicking the Download CSV button. I save my file into data/no2_gee_Delhi-India.csv of this Github repository. Conclusions It is similarly creating an average raster for the entire time domain of this dataset in the Map pane. Try zooming out, realizing that is happening globally. Now that‚Äôs big data! In a tiny amount of time, with minor effort. Further Resources Google Earth Engine GEE Lessons - GeoHackWeek: Software Carpentry style rgee: R package for GEE; still in early development (Gorelick et al. 2017) References "],
["sentiment.html", "Lesson 2 Sentiment 2.1 Twitter Examples 2.2 Authenticate Twitter 2.3 Enable Google Geocoding 2.4 city osm poly 2.5 openaq 2.6 aqicn.org 2.7 TensorFlow 2.8 tfhub", " Lesson 2 Sentiment 2.1 Twitter Examples ‚Äúair quality‚Äù - Twitter Search / Twitter {{% tweet \"1263487780799184897\" %}} 2.2 Authenticate Twitter Obtaining and using access tokens ‚Ä¢ rtweet: Creating a Twitter App Authorization methods Access token/secret method Authorization in future R sessions library(rtweet) get_token() 2.3 Enable Google Geocoding TODO: switch to OpenStreetMap API, for radius too To obtain an API key and enable services, go to https://cloud.google.com/maps-platform/. [as ben@ecoquants.com] Create project ‚Äúmeds-gee‚Äù Maps APIs and Servi‚Ä¶ ‚Äì Google Maps ‚Äì meds-gee ‚Äì Google Cloud Platform APIs sidebar. Click Geocoding API. Enable. Credentials sidebar. + CREATE CREDENTIALS -&gt; API key Saved to ~/private/google_meds-gee_api-key.txt # load libraries ---- # use librarian to load libraries, installing if needed if (!require(&quot;librarian&quot;)) install.packages(&quot;librarian&quot;) library(&quot;librarian&quot;) pkgs &lt;- c( # utility &quot;here&quot;,&quot;glue&quot;,&quot;stringr&quot;,&quot;dplyr&quot;,&quot;readr&quot;, # airquality &quot;ropensci/ropenaq&quot;, # spatial &quot;ggmap&quot;,&quot;mapview&quot;,&quot;leaflet&quot;, # text &quot;rtweet&quot;,&quot;textdata&quot;,&quot;tidytext&quot;, # tensorflow &quot;tensorflow&quot;,&quot;keras&quot;,&quot;tfhub&quot;,&quot;rstudio/tfds&quot;) shelf(pkgs) # variables ---- q &lt;- &#39;&quot;air quality&quot;&#39; city &lt;- &quot;Delhi, India&quot; city_r &lt;- &quot;15mi&quot; q_s &lt;- str_remove_all(q, &#39;[ ,&quot;]&#39;) city_s &lt;- str_replace(city, &quot;, &quot;, &quot;+&quot;) city_geo &lt;- glue(&quot;data/city_{city_s}.geojson&quot;) #cities_csv &lt;- glue(&quot;data/city_{loc_s}.csv&quot;) tweets_rds &lt;- glue(&quot;data/tweets_{q_s}_{city_s}.rds&quot;) #tbl_rds &lt;- glue(&quot;data/tweets_{qloc}.csv&quot;) #d_csv &lt;- &quot;data/tweets_air-quality.csv&quot; gkey &lt;- &quot;~/private/google_meds-gee_api-key.txt&quot; # get location lon/lat from place name if (!file.exists(loc_csv)){ register_google(key = readLines(gkey)) loc_xy &lt;- geocode(loc) stopifnot(nrow(loc_xy) == 1) write_as_csv(loc_xy, loc_csv) } loc_xy &lt;- read_csv(loc_csv) loc_xyr &lt;- glue(&quot;{loc_xy$lat},{loc_xy$lon},{loc_r}&quot;) if (!file.exists(tbl_rds)){ tbl &lt;- search_tweets( q, n = 1000, geocode = loc_xyr) saveRDS(tbl, tbl_rds) } tbl &lt;- readRDS(tbl_rds) s_b &lt;- get_sentiments(&#39;bing&#39;) s_a &lt;- get_sentiments(&#39;afinn&#39;) s_n &lt;- get_sentiments(&#39;nrc&#39;) tbl &lt;- tbl %&gt;% mutate( text_clean = text %&gt;% str_replace_all(&quot;[^[:ascii:]]&quot;, &quot;_&quot;) %&gt;% tolower() %&gt;% str_replace_all(&quot;@[^ ]+&quot;, &quot;_usr_&quot;) %&gt;% str_replace_all(&quot;http[^ ]+&quot;, &quot;_url_&quot;)) # tbl %&gt;% select(created_at, screen_name, text, text_clean) %&gt;% View() df &lt;- tbl %&gt;% select(status_id, created_at, screen_name, text_clean) %&gt;% unnest_tokens(output = word, input = text_clean, token = &quot;words&quot;) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% left_join(s_b, by = &quot;word&quot;) 2.4 city osm poly source(&quot;functions.R&quot;) if (!file.exists(city_geo)) city2geo(city_s, city_geo) city_ply &lt;- read_sf(city_geo) write_sf(city_ply, &quot;data/city_delhi/delhi.shp&quot;) # https://developers.google.com/earth-engine/charts_image_series mapview(city_ply) 2.5 openaq API: https://docs.openaq.org/#api-Measurements-GetMeasurements ropenaq: Accesses Air Quality Data from the Open Data Platform OpenAQ library(ropenaq) library(tidyr) cities_table &lt;- aq_cities() %&gt;% filter(str_detect(city,&quot;Delhi&quot;)) aq_cities() %&gt;% filter(str_detect(city,&quot;Delhi&quot;)) city_locs &lt;- aq_locations(country = &quot;IN&quot;, city = &quot;Delhi&quot;) %&gt;% filter(no2, pm25) %&gt;% arrange(desc(count)) View(city_locs) loc_d &lt;- city_locs %&gt;% select(-count) %&gt;% unnest(countsByMeasurement) %&gt;% filter( parameter == &quot;no2&quot;, date(lastUpdated) &gt; now() %&gt;% date() - days(1)) %&gt;% arrange(desc(count)) %&gt;% slice(1) loc &lt;- loc_d$location loc &lt;- loc_d$location %&gt;% str_replace_all(&quot; &quot;, &quot;+&quot;) city_loc_d &lt;- city_locs %&gt;% filter(location == loc) city_loc_d %&gt;% View() city_locs %&gt;% filter(location == loc) %&gt;% View() m_no2 &lt;- aq_measurements(country = &quot;IN&quot;, city = &quot;Delhi&quot;, parameter = &quot;no2&quot;) range(as_date(m_no2$dateUTC)) # &quot;2019-12-12&quot; &quot;2020-05-23&quot; attr(m_no2, &quot;meta&quot;) # limit: 10,000; found: 1,162,354 m_no2 &lt;- aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;10&quot;, parameter = &quot;no2&quot;, page = 1) %&gt;% bind_rows( aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;100&quot;, parameter = &quot;no2&quot;, page = 2)) %&gt;% bind_rows( aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;100&quot;, parameter = &quot;no2&quot;, page = 3)) %&gt;% bind_rows( aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;100&quot;, parameter = &quot;no2&quot;, page = 4)) range(as_date(m_no2$dateUTC)) # &quot;2019-12-12&quot; &quot;2020-05-23&quot; attr(m_no2, &quot;meta&quot;) # limit: 10,000; found: 35,069 m1 &lt;- aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;100&quot;, parameter = &quot;no2&quot;, page = 1) range(as_date(m1$dateUTC)) # &quot;2020-02-24&quot; &quot;2020-05-23&quot; attr(m1, &quot;meta&quot;) # limit: 10,000; found: 35,069 m2 &lt;- aq_measurements( latitude = loc_d$latitude, longitude = loc_d$longitude, radius = &quot;100&quot;, parameter = &quot;no2&quot;, page = 2) range(as_date(m2$dateUTC)) # attr(m2, &quot;meta&quot;) # limit: 10,000; found: location &lt;- https://api.openaq.org/v1/locations q_loc &lt;- loc_d$location %&gt;% str_replace_all(&quot; &quot;, &quot;+&quot;) q_loc fromJSON(glue(&quot;https://api.openaq.org/v1/locations?location={q_loc}&quot;)) openaq https://api.openaq.org/v1/measurements?location=Shadipur%2C%2BDelhi%2B-%2BCPCB&amp;parameter=no2 https://api.openaq.org/v1/locations/IN-88 https://api.openaq.org/v1/locations?location=Shadipur,+Delhi+-+CPCB https://api.openaq.org/v1/measurements?location=Shadipur,+Delhi+-+CPCB&amp;parameter=no2 library(httr) m_url &lt;- &quot;https://api.openaq.org/v1/measurements&quot; json_url &lt;- glue(&quot;https://api.openaq.org/v1/measurements?location={q_loc}&amp;parameter=no2&amp;limit=1&amp;format=json&quot;) x &lt;- GET( m_url, query = list( location = loc_d$location, parameter = &quot;no2&quot;, format = &quot;json&quot;, limit = 1)) meta &lt;- content(x, &quot;text&quot;) %&gt;% fromJSON() %&gt;% .$meta n_max &lt;- 10000 n_pages &lt;- ceiling(meta$found/n_max) for (i in 1:pages){ # i = 2 date_beg &lt;- loc_d$firstUpdated %&gt;% as_date() %&gt;% as.character() date_end &lt;- loc_d$firstUpdated + days(60) date_end &lt;- as_date(date_end) %&gt;% as.character() date_beg &lt;- date(&quot;2018-01-01&quot;) date_end &lt;- date_beg + months(1) x &lt;- GET( m_url, query = list( #location = loc_d$location, coordinates = with(city_ply, glue(&quot;{round(lat,2)},{round(lon,2)}&quot;)), #radius = 2500, # default radius = 1000000, # default parameter = &quot;no2&quot;, format = &quot;csv&quot;, #format = &quot;json&quot;, date_from = date_beg, date_to = date_end, #limit = n_max, limit = 5, page = i)) x$url #if (http_type(x) != &quot;application/json&quot;) # stop(&quot;API did not return json&quot;, call. = F) #cat(content(x, &quot;text&quot;)) # {&quot;statusCode&quot;:404,&quot;error&quot;:&quot;Not Found&quot;} # no2 unit: ¬µg/m¬≥ #y &lt;- fromJSON(content(x, &quot;text&quot;)) y &lt;- read_csv(content(x, &quot;text&quot;)) y y %&gt;% select(dtime_utc = utc, no2_mgm3 = value) %&gt;% write_csv(&quot;data/aq_delhi_no2.csv&quot;, append = ifelse(i != 1, T, F)) # 6636 } d &lt;- read_csv(url) %&gt;% .$results tbl &lt;- res$results %&gt;% as_tibble() names(tbl) tbl %&gt;% select(parameter, date_utc = tbl$date$) names(res$results) m_no2_a &lt;- aq_measurements( country = &quot;IN&quot;, city = &quot;Delhi&quot;, parameter = &quot;no2&quot;, attribution = T, averaging_period = T, source_name = T) aq_locations(country = &quot;IN&quot;, city = &quot;Delhi&quot;) m_no2_loc &lt;- aq_measurements( #country = &quot;IN&quot;, city = &quot;Delhi&quot;, location = &quot;Shadipur, New Delhi - CPCB&quot;, #country = &quot;IN&quot;, city = &quot;Delhi&quot;, location = loc, #location = &quot;DTU, Delhi - CPCB&quot;, location = &quot;DTU,+Delhi+-+CPCB&quot;, #country = &quot;IN&quot;, city = &quot;Delhi&quot;, location = &quot;IN-88&quot;, parameter = &quot;no2&quot;) range(as_date(m_no2_loc$dateUTC)) # attr(m_no2_loc, &quot;meta&quot;) # limit: 10,000; found: aq_measurements tmp &lt;- aq_measurements(country=&quot;IN&quot;, city=&quot;Delhi&quot;, location=&quot;Anand+Vihar&quot;, parameter=&quot;pm25&quot;) coordinates, radius date_from date_to m_pm25 &lt;- aq_measurements(country = &quot;IN&quot;, city = &quot;Delhi&quot;, parameter = &quot;pm25&quot;) attr(m, &quot;meta&quot;) # limit: 10,000; found: 1,141,521 range(as_date(m_pm25$dateUTC)) # &quot;2019-12-12&quot; &quot;2020-05-23&quot; #, limit = 10, page = 1) kable(results_table) 2.6 aqicn.org Downloaded from: - Air Quality Historical Data Platform aqicn.org/city/delhi - Delhi Air Pollution: Real-time Air Quality Index; daily avg of this station: - Anand Vihar, Delhi sources: - Delhi Pollution Control Commitee (Government of NCT of Delhi) - CPCB - India Central Pollution Control Board TODO: Beijing Air Pollution: Real-time Air Quality Index COVID-19 Worldwide Air Quality data 380 major cities in the world, from January 2020 until now. The CSV data sets can be downloaded programatically: The url is https://aqicn.org/data-platform/covid19/report/14286-3d64290c/period Where period is any of 2019Q1, 2019Q2, 2019Q3, 2019Q4, 2018H1, 2017H1, 2016H1, 2015H1. For instance using curl: curl ‚Äìcompressed -o waqi-covid-2020.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2020 curl ‚Äìcompressed -o waqi-covid-2019Q1.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2019Q1 curl ‚Äìcompressed -o waqi-covid-2019Q2.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2019Q2 curl ‚Äìcompressed -o waqi-covid-2019Q3.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2019Q3 curl ‚Äìcompressed -o waqi-covid-2019Q4.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2019Q4 curl ‚Äìcompressed -o waqi-covid-2018H1.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2018H1 curl ‚Äìcompressed -o waqi-covid-2017H1.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2017H1 curl ‚Äìcompressed -o waqi-covid-2016H1.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2016H1 curl ‚Äìcompressed -o waqi-covid-2015H1.csv https://aqicn.org/data-platform/covid19/report/14286-3d64290c/2015H1 To download the list of stations with latitude, longitude and EPA feed: curl ‚Äìcompressed -o airquality-covid19-cities.json https://aqicn.org/data-platform/covid19/airquality-covid19-cities.json library(dygraphs) library(lubridate) library(xts) #d &lt;- read_csv(&quot;data/new-delhi us embassy, india-air-quality.csv&quot;) %&gt;% d &lt;- read_csv(&quot;data/anand-vihar, delhi, delhi, india-air-quality.csv&quot;) %&gt;% mutate( date = as_date(date)) d # date pm25 pm10 o3 no2 so2 co #View(d) range(d$date) #range(d$jday) d &lt;- d %&gt;% mutate( jday = yday(date)) # calculate climatology d_c &lt;- d %&gt;% group_by(jday) %&gt;% summarise(no2_clim = mean(no2, na.rm = T)) %&gt;% ungroup() # show in time d_y &lt;- expand_grid( year = min(year(d$date)):year(now()), jday = 1:365) %&gt;% mutate( date = date(glue(&quot;{year}-01-01&quot;)) + days(jday - 1)) %&gt;% left_join(d_c, by = &quot;jday&quot;) %&gt;% filter(date &lt;= date(now())) d_y2 &lt;- d_y %&gt;% select(date, no2_clim) %&gt;% left_join( d %&gt;% select(date, no2), by = &quot;date&quot;) #x &lt;- xts(x = d$pm25, order.by = d$date) x &lt;- xts(x = select(d_y2, -date), order.by = pull(d_y2, date)) dygraph(x) %&gt;% dyRangeSelector() Next steps: weekly / monthly average RPubs - Beijing PM 2.5 Line Charts 2017/01 to 2018/03 ## search full archive Setup dev env: Dev environments ‚Äî Twitter Developers beg &lt;- &quot;2020-05-20&quot; end &lt;- &quot;2020-05-23&quot; tbl &lt;- search_fullarchive( q, n = 100, fromDate = beg, toDate = end, env_name = &quot;research&quot;) # safedir = NULL, # parse = TRUE, # token = NULL # point_radius:[lon lat radius]) table(as.Date(tbl$created_at)) ts_plot(tbl) fromDate, toDate, /search/fullarchive/:label.json TODO: get sentiment per tweet by tallying negative &amp; positive join back to tweets get over time 2.7 TensorFlow datasets to train on: Sentiment140 dictionary.txt in Sentiment analysis for text with Deep Learning - Towards Data Science from https://nlp.stanford.edu/sentiment/ Sentiment analysis üëç üëé on Twitter using Word2vec and Keras | Ahmed BESBES: &quot;The dataset can be downloaded from this link. It‚Äôs a csv file that contains 1.6 million rows. Each row has amongst other things the text of the tweet and the corresponding sentiment. covid-twitter-bert | TensorFlow Hub Sentiment140 dataset with 1.6 million tweets | Kaggle http://help.sentiment140.com/ Sentiment140 allows you to discover the sentiment of a brand, product, or topic on Twitter. http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip 2.8 tfhub RStudio AI Blog: tfhub: R interface to TensorFlow Hub you need to install the Python packages once: tensorflow::install_tensorflow() keras::install_keras() tfhub::install_tfhub() tfds::install_tfds() reticulate::py_config() library(tfhub) library(keras) layer_mobilenet &lt;- layer_hub( handle = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4&quot;) input &lt;- layer_input(shape = c(224, 224, 3)) output &lt;- layer_mobilenet(input) model &lt;- keras_model(input, output) summary(model) {rv}, eval = Fimg &lt;- image_load(&quot;data/grace-hopper.jpg&quot;, target_size = c(224,224)) %&gt;% image_to_array() img &lt;- img/255 dim(img) &lt;- c(1, dim(img)) pred &lt;- predict(model, img) imagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]] `` "]
]
