# Sentiment

```{r, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```

## STATUS: NOT READY {-}

Sorry, this teaching module is not ready for students to run through just yet. I experienced a couple issues:

1. The free Twitter developer API access only goes back 1 week. I upgraded to Premium for the Full Archive search going back to Twitter beginnings in 2006, but couldn't get the [`rtweet::search_fullarchive()`](https://rtweet.info/reference/search_fullarchive.html) to return any results. I finally got a week's worth of results with a more manual approach,  [_tweet-premium.R](https://github.com/bbest/meds-demo/blob/master/_tweet-premium.R), but then became time constrained to get the rest.

1. I wanted to use an existing trained model, i.e. "transfer learning", to improve accuracy and not get into the weeds of convolutional neural net model design, but had trouble getting [tfhub](https://tensorflow.rstudio.com/guide/tfhub/intro/) to work.

## Objectives {- .objectives}

### Question {-}

How has sentiment around air quality and clean energy changed since air became cleaner? 

**Background**. In anticipation of the 2008 Summer Olympics in Beijing the Chinese government imposed strict rules to clean up the air. The people got used to better air and now Beijing is consistently improved. Will this hiatus to bad air be temporary or incite a push towards cleaner energy. For a compelling summary, check out the podcast [The Natural Experiment - 99% Invisible](https://99percentinvisible.org/episode/the-natural-experiment/).

### Technical Motivation {-}

Sentiment can be evaluated as either positive or negative. This **binary classification** is the most basic response for **machine learning**, thus a good example for a lesson on machine learning. Text however can have many complicated forms, such as negating terms (eg "happy" vs "_**not**_ happy"), which makes it a good candidate for **deep learning**.

### Approach {-}

1. Get a set of sample tweets before and after the lockdown with some of key terms.
1. Lookup tweeted words with dictionaries labelling positive or negative, and tally the score. Bren's own Casey O'Hara & Jessica Couture already explained this approach well in their Eco-Data-Science workshop [Text analysis with R](https://github.com/oharac/text_workshop) (2019-02-05).
1. Introduce TensorFlow starting with a pre-trained model.
1. Use a sample of the Sentiment140 twitter dataset to train an existing natural language processing (NLP) model.
1. Predict that over time.

## Prerequisites {- .prereq}

A **Twitter developer account** is required to download tweets and access the https://developer.twitter.com/en/dashboard. You'll need to apply via the [Twitter developer signup](https://developer.twitter.com/en/apply-for-access).

I recieved an email for clarification and had the account approved and running by the end of the day.

## Setup Twitter token

We'll use the [rtweet](https://rtweet.info) R package to get twitter data. You'll need to setup an access token though after applying for a Twitter API

See [Obtaining and using access tokens • rtweet](https://rtweet.info/articles/auth.html):

- Creating a Twitter App
- Authorization methods
  - 2. Access token/secret method
- Authorization in future R sessions
  ```r
  library(rtweet)
  get_token()
  ```
  
For `search_fullarchive` also tried to setup [Dev environment — Twitter Developers](https://developer.twitter.com/en/account/environments), but that didn't seem to work.

## Load R packages

```{r}
# load libraries ----
# use librarian to load libraries, installing if needed
if (!require("librarian")) install.packages("librarian")
library("librarian")

pkgs <- c(
  # utility
  "here","glue","stringr","dplyr","readr","ggplot2","purrr",
  # airquality
  #"ropensci/ropenaq",
  # spatial
  "sf",#"ggmap","mapview","leaflet",
  # text
  "rtweet","tidytext","textdata",
  # tensorflow
  "tensorflow","keras","tfhub","rstudio/tfds","pins")
shelf(pkgs)
```

## Search Twitter

Using hashtags from [@gurajalaUnderstandingPublicResponse2019]: #AIRPOLLUTION #AIRQUALITY #CLEANAIR #HAZE #OZONE #PARTICLES #PARTICULATES #PM25 #PM2.5 #PM10 #POLLUTION #SMOG #EMISSIONS 

```{r}
city_geo <- here("data/city_Delhi-India.geojson")
now_rds  <- here("data/twitter_aq_delhi_now.rds")
yr1_rds  <- here("data/twitter_aq_delhi_1yr.rds")

aq_hashes <- c("#AIRPOLLUTION #AIRQUALITY #CLEANAIR #HAZE #OZONE #PARTICLES #PARTICULATES #PM25 #PM2.5 #PM10 #POLLUTION #SMOG #EMISSIONS") %>% 
  str_replace_all(" ", " OR ")

q_bb <- read_sf(city_geo) %>% 
  st_bbox() %>% 
  glue_data(
  "bounding_box:[{xmin} {ymin} {xmax} {ymax}]")

geocode_str <- read_sf(city_geo) %>% 
  glue_data(
  "{round(lat, 4)},{round(lon, 4)},{round(r_mi*2, 2)}mi")

q_geo <- read_sf(city_geo) %>% 
  glue_data(
  "point_radius:[{round(lon, 4)} {round(lat, 4)} {round(r_mi*2, 2)}mi]")

if (!file.exists(now_rds)){
  tbl <- search_tweets(
    q       = aq_hashes,
    geocode = geocode_str,
    n       = 1000)
  
  saveRDS(tbl, now_rds)
}

if (!file.exists(yr1_rds)){
#if (F){
  tbl_yr1 <- search_fullarchive(
    env_name = "research",
    fromDate = "201905190000",
    toDate   = "201905260000",
    q        = glue("({aq_hashes}) {q_geo}"),
    n        = 1000)
  
  saveRDS(tbl_yr1, yr1_rds)
}
```

## Calculate dictionary score

```{r}
s_b <- get_sentiments('bing')
# s_a <- get_sentiments('afinn')
# s_n <- get_sentiments('nrc')

tbl <- readRDS(now_rds)

# clean out non-ascii, twitter handles, and urls
tbl <- tbl %>% 
  mutate(
    text_clean = text %>% 
      str_replace_all("[^[:ascii:]]", "_") %>% 
      tolower() %>% 
      str_replace_all("@[^ ]+", "_usr_") %>% 
      str_replace_all("http[^ ]+", "_url_"))

# tweets to words
words <- tbl %>% 
  select(status_id, created_at, screen_name, text_clean) %>% 
  unnest_tokens(output = word, input = text_clean, token = "words") %>% 
  anti_join(stop_words, by = "word") %>% 
  left_join(s_b, by = "word") %>% 
  left_join(
    tribble(
      ~sentiment, ~score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")

# tally score per tweet
tbl <- tbl %>% 
  left_join(
    words %>% 
      group_by(status_id) %>% 
      summarize(
        score = mean(score, na.rm = T)),
    by = "status_id")

hist(tbl$score)
mean(na.omit(tbl$score))
nrow(tbl)
```

## Twitter test dataset

[For Academics - Sentiment140 - A Twitter Sentiment Analysis Tool](http://help.sentiment140.com/for-students)

```{r}
s140_csv <- here("data/sentiment140_testdata.manual.2009.06.14.csv")
s140 <- read_csv(
  s140_csv, col_names = c(
    "polarity", "status_id", "created_at", "query", "screen_name", "text")) %>% 
  mutate(
    # convert negative 0 -> -1, neutral 2 -> 0, positive 4 -> 1  
    polarity = recode(polarity, `0` = -1, `2` = 0, `4` = 1))

# clean out non-ascii, twitter handles, and urls
s140 <- s140 %>% 
  mutate(
    text_clean = text %>% 
      str_replace_all("[^[:ascii:]]", "_") %>% 
      tolower() %>% 
      str_replace_all("@[^ ]+", "_usr_") %>% 
      str_replace_all("http[^ ]+", "_url_"))

# tweets to words
words <- s140 %>% 
  select(status_id, created_at, screen_name, text_clean) %>% 
  unnest_tokens(output = word, input = text_clean, token = "words") %>% 
  anti_join(stop_words, by = "word") %>% 
  left_join(s_b, by = "word") %>% 
  left_join(
    tribble(
      ~sentiment, ~score,
      "positive",  1,
      "negative", -1),
    by = "sentiment")

# tally score per tweet
s140 <- s140 %>% 
  left_join(
    words %>% 
      group_by(status_id) %>% 
      summarize(
        score = mean(score, na.rm = T)),
    by = "status_id")

hist(s140$score)
nrow(s140)
mean(na.omit(s140$score))

# performance
s140 <- s140 %>% 
  mutate(
    accurate_dict = case_when(
      polarity == -1 & score < 0 ~ T,
      polarity ==  0 & score == 0 ~ T,
      polarity ==  1 & score > 0 ~ T,
      T ~ F))
select(s140, polarity, score, accurate_dict)
sum(s140$accurate_dict / nrow(s140))
```

## Introducing TensorFlow

[RStudio AI Blog: tfhub: R interface to TensorFlow Hub](https://blogs.rstudio.com/ai/posts/2019-12-18-tfhub-0.7.0/)

You need to install these Python packages once:

```{r, eval=F}
tensorflow::install_tensorflow()
keras::install_keras()
tfhub::install_tfhub()
tfds::install_tfds()
reticulate::py_config()
```

### Text classification

[Text Classification](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)

The dataset was downloaded from [Movie Reviews | Kaggle]( https://www.kaggle.com/nltkdata/movie-review#movie_review.csv).

```{r}
#df <- read_csv(here("data/movie_review.csv"))
df <- s140 %>%
  mutate(
    # convert negative -1 -> neg, neutral 0 -> neu, positive 1 -> pos
    tag = recode(polarity, `-1` = "neg", `0` = "neu", `1` = "pos"))
df %>% count(tag)
df$text[1]

# split our dataset into training and testing
training_id <- sample.int(nrow(df), size = nrow(df)*0.8)
training    <- df[training_id,]
testing     <- df[-training_id,]

# distribution of number of words in each review?
#df$text %>% 
df$text_clean %>% 
  strsplit(" ") %>% 
  sapply(length) %>% 
  summary()

# create padded arrays
num_words <- 10000
max_length <- 50
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length,)

text_vectorization %>% 
  #adapt(df$text)
  adapt(df$text_clean)

# TODO see https://github.com/tensorflow/tensorflow/pull/34529
get_vocabulary(text_vectorization) %>% head(20)

text_vectorization(matrix(df$text[1], ncol = 1))

input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)

# configure the model to use an optimizer and a loss function
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy'))

# train the model
history <- model %>% fit(
  training$text,
  as.numeric(training$tag == "pos"),
  epochs = 10,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2)

# evaluate the model
results <- model %>% evaluate(testing$text, as.numeric(testing$tag == "pos"), verbose = 0)
results

# graph accuracy and loss over iterations
plot(history)

results
```

### not working on Ben's laptop: trained model on image classification

```{r, eval = F}
library(tfhub)
library(keras)

layer_mobilenet <- layer_hub(
  handle = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4")

input  <- layer_input(shape = c(224, 224, 3))
output <- layer_mobilenet(input)
model  <- keras_model(input, output)
summary(model)
```

The `layer_hub()` function above however kept throwing errors like this:

```
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  OSError: SavedModel file does not exist at: /var/folders/2r/grqvdjfn04361tzk8mh60st40000gn/T/tfhub_modules/426589ad685896ab7954855255a52db3442cb38d/{saved_model.pbtxt|saved_model.pb}
```

There's a lot of Python and R communication that can get easily confused between versions. So let's switch to a clean installation by using Docker. [Install Docker](https://docs.docker.com/get-docker/) if you don't already have it.

Then per [rocker/tensorflow - Docker Hub](https://hub.docker.com/r/rocker/tensorflow) run the following to get a clean RStudio instance with all the TensorFlow and Python dependencies properly installed:

```bash
docker run -e PASSWORD=mu -p 8787:8787 --name ml4r rocker/ml 
```

Visit http://localhost:8787/ and enter username rstudio, password mu.

Then in the Terminal of RStudio run:

```
git clone https://github.com/bbest/meds-demo
```

```{r, eval = F}
img <- image_load("data/grace-hopper.jpg", target_size = c(224,224)) %>% 
image_to_array()
img <- img/255
dim(img) <- c(1, dim(img))
pred <- predict(model, img)
imagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]]
```
